here you go, this is part 1/2. These are the most important files, I left out the dockerfiles and datasets etc.. Important to know is that I did not write the crca and cgnn modules, it is not the major part of my project. I created the framework, so the flask etc. Again for reference:cgnn is  Correlative-GNN with Multi-Head Self-Attention and Auto-Regression Ensemble Method (CGNN-MHSA-AR) for unsupervised multi- variate time series anomaly detection in distributed applications. This method leverages the power of deep learning techniques, including GNN, multi-head self- attention, and auto-regression, to achieve accurate anomaly detection without the need of labeled data. crca is a root cause localization framework working with causal inference and named CausalRCA. The CausalRCA uses a gradient-based causal structure learning method to generate weighted causal graphs and a root cause inference method to localize root cause metrics, achieving fine-grained, automated, and real-time root cause localization.

```data_ingestion/app.py
from flask import Flask, request, jsonify
from celery import Celery
import logging
import requests
import time
import json
import pandas as pd
from kubernetes import client, config
import traceback
from flask_cors import CORS

from data_collector import collect_crca_data, fetch_metrics
from config import set_initial_metric_config, get_config, set_config

app = Flask(__name__)
CORS(app, resources={r"/*": {"origins": "*"}})
app.config['CELERY_BROKER_URL'] = 'redis://redis:6379/0'
app.config['CELERY_RESULT_BACKEND'] = 'redis://redis:6379/0'

celery = Celery(app.name, broker=app.config['CELERY_BROKER_URL'])
celery.conf.update(app.config)

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# celery -A app_1.celery worker --loglevel=info


@celery.task(bind=True)
def monitoring_task(self, monitor_info):
    iteration = 0
    test_info = monitor_info
    test_info['task_id'] = self.request.id



    while True:
        # Check for task revocation by querying the backend
        task = monitoring_task.AsyncResult(self.request.id)
        if task.state == 'REVOKED':
            break

        end_time = int(time.time())
        start_time = end_time - (test_info['data']['duration'] * 60)
        dataframe = fetch_metrics(test_info['data']['pods'], test_info['data']['metrics'], start_time, end_time,
                                  test_info['settings']['PROMETHEUS_URL'], test_info['data']['data_interval'])
        test_files = {'test_array': dataframe.to_csv(header=False, index=False)}
        test_info['data']['start_time'] = start_time
        test_info['data']['end_time'] = end_time
        test_info['data']['iteration'] = iteration
        test_info_json = json.dumps(test_info)
        print(test_info_json)
        requests.post(f"{test_info['settings']['API_DATA_PROCESSING_URL']}/preprocess_cgnn_data",
                      files=test_files, data={'test_info': test_info_json})
        time.sleep(test_info['data']['test_interval'] * 60)
        iteration += 1


@app.route('/start_monitoring', methods=['POST'])
def start_monitoring():
    monitor_info_json = request.form.get('monitor_info')
    monitor_info = json.loads(monitor_info_json)
    task = monitoring_task.apply_async(args=[monitor_info])
    logging.info(f"Task {task.id} started")
    return jsonify({'status': 'monitoring_started', 'task_id': task.id}), 200


@app.route('/get_active_tasks', methods=['GET'])
def get_tasks():
    active_tasks = celery.control.inspect().active()
    return jsonify(active_tasks), 200


@app.route('/stop_monitoring/<task_id>', methods=['DELETE'])
def stop_monitoring(task_id):
    if task_id:
        celery.control.revoke(task_id, terminate=True)
        return jsonify({'status': f'monitoring_stopped for task_id {task_id}'}), 200
    else:
        return jsonify({'status': 'task_id_missing'}), 400


@app.route('/anomaly_rca', methods=['POST'])
def anomaly_rca():
    try:
        data = request.form.get('crca_data')
        data = json.loads(data)
        response = collect_crca_data(data)
        return response
    except Exception as e:
        logger.error(traceback.format_exc())
        return jsonify({"error": str(e)}), 500


@app.route('/get_pod_names', methods=['POST'])
def get_pod_names():
    """
    List the names of the pods in the specified namespace.

    Returns:
        str: Message indicating that monitoring has started.
    """
    try:
        namespace = request.json['namespace']
        v1 = client.CoreV1Api()
        ret = v1.list_namespaced_pod(namespace, watch=False)
        pod_names = [item.metadata.name for item in ret.items]
        return jsonify(pod_names), 200
    except Exception as e:
        pod_names = ['hoi', 'doei']
        return jsonify(pod_names), 500
        # return jsonify({"error": str(e)}), 500

@app.route('/load_kube_config', methods=['POST'])
def load_config():
    try:
        # config.load_kube_config(request.json['kube_config_path'])
        config.load_incluster_config()
        return jsonify({"message": "Kubernetes configuration loaded successfully."}), 200
    except FileNotFoundError as e:
        logging.error("Kubernetes configuration file not found: %s", str(e))
        return jsonify({"error": "Kubernetes configuration file not found"}), 400

    except config.ConfigException as e:
        logging.error("Error loading Kubernetes configuration: %s", str(e))
        return jsonify({"error": "Invalid Kubernetes configuration"}), 400

    except ConnectionError as e:
        logging.error("Could not connect to Kubernetes cluster: %s", str(e))
        return jsonify({"error": "Could not connect to Kubernetes cluster"}), 503

    except Exception as e:
        logging.error("Unexpected error: %s", str(e))
        return jsonify({"error": "Internal Server Error"}), 500


@app.route('/get_metrics', methods=['GET'])
def get_config_route():
    try:
        config = get_config()
        return jsonify(config), 200
    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route('/update_config', methods=['POST'])
def update_config():
    """
    Updates the configuration for the monitoring application.
    """
    new_config = request.json
    try:
        set_config(new_config)
        return jsonify("success"), 200
    except Exception as e:
        return jsonify({"error": str(e)}), 500


if __name__ == '__main__':
    set_initial_metric_config()
    app.run(debug=True, host='0.0.0.0', port=5001)
```
```data_ingestion/config.py
# config.py
import json

config_file_path = "metric_config.json"


def get_config():
    with open(config_file_path, 'r') as file:
        config = json.load(file)

    return config


def set_config(new_config=None):
    config = get_config()
    if new_config:
        config.update(new_config)
    with open(config_file_path, 'w') as file:
        json.dump(config, file, indent=2)


def set_initial_metric_config():
    config = {
        "cpu_usage_pods": {
            "query": 'sum(rate(container_cpu_usage_seconds_total{{pod="{pod}"}}[1m])) by (pod)',
            "info": "CPU usage"
        },
        "memory_usage_pods": {
            "query": 'sum(container_memory_usage_bytes{{pod="{pod}"}}) by (pod)',
            "info": "Memory usage"
        },
        "cpu_limit_pods": {
            "query": 'sum(kube_pod_container_resource_limits_cpu_cores{{pod="{pod}"}}) by (pod)',
            "info": "CPU limit"
        },
        "memory_limit_pods": {
            "query": 'sum(kube_pod_container_resource_limits_memory_bytes{{pod="{pod}"}}) by (pod)',
            "info": "Memory limit"
        },
        "cpu_requests_pods": {
            "query": 'sum(kube_pod_container_resource_requests_cpu_cores{{pod="{pod}"}}) by (pod)',
            "info": "CPU requests"
        },
        "memory_requests_pods": {
            "query": 'sum(kube_pod_container_resource_requests_memory_bytes{{pod="{pod}"}}) by (pod)',
            "info": "Memory requests"
        },
        "pod_restarts": {
            "query": 'sum(rate(kube_pod_container_status_restarts_total{{pod="{pod}"}}[1m])) by (pod)',
            "info": "Number of restarts"
        },
        "pod_status_running": {
            "query": 'sum(kube_pod_status_phase{{pod="{pod}", phase="Running"}}) by (pod)',
            "info": "Number of pods in running state"
        },
        "pod_status_pending": {
            "query": 'sum(kube_pod_status_phase{{pod="{pod}", phase="Pending"}}) by (pod)',
            "info": "Number of pods in pending state"
        },
        "pod_status_failed": {
            "query": 'sum(kube_pod_status_phase{{pod="{pod}", phase="Failed"}}) by (pod)',
            "info": "Number of pods in failed state"
        },
        "pod_status_succeeded": {
            "query": 'sum(kube_pod_status_phase{{pod="{pod}", phase="Succeeded"}}) by (pod)',
            "info": "Number of pods in succeeded state"
        },
        "disk_io_operations": {
            "query": 'sum(rate(container_fs_reads_bytes_total{{pod="{pod}"}}[1m])) by (pod)',
            "info": "Disk I/O operations"
        },
        "network_io_bytes_received": {
            "query": 'sum(rate(container_network_receive_bytes_total{{pod="{pod}"}}[1m])) by (pod)',
            "info": "Network bytes received by pods"
        },
        "network_io_bytes_transmitted": {
            "query": 'sum(rate(container_network_transmit_bytes_total{{pod="{pod}"}}[1m])) by (pod)',
            "info": "Network bytes transmitted by pods"
        },
        "pod_evictions": {
            "query": 'sum(rate(kube_pod_evictions_total{{pod="{pod}"}}[1m])) by (pod)',
            "info": "Number of pod evictions"
        },
        "pod_uptime": {
            "query": 'time() - kube_pod_start_time{{pod="{pod}"}}',
            "info": "Uptime of pod"
        },
        "pods_by_node": {
            "query": 'sum(kube_pod_info{{pod="{pod}"}}) by (node)',
            "info": "Number of pods by node"
        },
        "pod_cpu_throttling": {
            "query": 'sum(rate(container_cpu_cfs_throttled_seconds_total{{pod="{pod}"}}[1m])) by (pod)',
            "info": "CPU throttling"
        },
        "pod_oom_kills": {
            "query": 'sum(rate(kube_pod_container_status_oomkilled{{pod="{pod}"}}[1m])) by (pod)',
            "info": "Out Of Memory kills"
        },
        "pod_read_bytes": {
            "query": 'sum(rate(container_fs_reads_bytes_total{{pod="{pod}"}}[1m])) by (pod)',
            "info": "Read bytes"
        },
        "pod_write_bytes": {
            "query": 'sum(rate(container_fs_writes_bytes_total{{pod="{pod}"}}[1m])) by (pod)',
            "info": "Write bytes"
        },
        "pod_log_size": {
            "query": 'sum(container_fs_usage_bytes{{pod="{pod}", device="log"}}) by (pod)',
            "info": "Log size of pod"
        },
        "pod_fs_inodes_free": {
            "query": 'sum(container_fs_inodes_free{{pod="{pod}"}}) by (pod)',
            "info": "Free inodes"
        },
        "pod_fs_inodes_total": {
            "query": 'sum(container_fs_inodes_total{{pod="{pod}"}}) by (pod)',
            "info": "Total inodes"
        },
        "pod_fs_inodes_used": {
            "query": 'sum(container_fs_inodes_used{{pod="{pod}"}}) by (pod)',
            "info": "Used inodes"
        },
        "pod_disk_space_available": {
            "query": 'sum(container_fs_available_bytes{{pod="{pod}"}}) by (pod)',
            "info": "Available disk space"
        },
        "pod_network_errors": {
            "query": 'sum(rate(container_network_receive_errors_total{{pod="{pod}"}}[1m])) by (pod)',
            "info": "Network errors"
        },
        "pod_network_dropped_packets": {
            "query": 'sum(rate(container_network_receive_packets_dropped_total{{pod="{pod}"}}[1m])) by (pod)',
            "info": "Network dropped packets"
        }
    }

    with open(config_file_path, 'w') as file:
        json.dump(config, file, indent=2)
```
```data_ingestion/data_collector.py
import requests
import logging
import pandas as pd
import time
import json
from flask import Response

from config import get_config

logging.basicConfig(level=logging.INFO)




def collect_crca_data(crca_data):
    dataframe = fetch_metrics(crca_data['data']['crca_pods'], crca_data['data']['metrics'],
                              crca_data['data']['start_time'], crca_data['data']['end_time'],
                              crca_data['settings']['PROMETHEUS_URL'], crca_data['data']['step'])
    crca_file = {'crca_file': dataframe.to_csv(header=False, index=False)}
    # crca_file = {'crca_file': open('data_full.csv', 'rb')}
    crca_info_json = json.dumps(crca_data)
    response = requests.post(f"{crca_data['settings']['API_DATA_PROCESSING_URL']}/preprocess_crca_data",
                             files=crca_file, data={'crca_info': crca_info_json})
    flask_response = Response(
        response.content,
        status=response.status_code,
        content_type=response.headers['Content-Type']
    )

    return flask_response


def fetch_metrics(pod_names, metric_queries, start_time, end_time, url, step=60):
    all_timestamps = pd.date_range(
        start=pd.Timestamp.fromtimestamp(start_time),
        end=pd.Timestamp.fromtimestamp(end_time),
        freq=f'{step}s'
    )
    master_df = pd.DataFrame(all_timestamps, columns=['timestamp'])
    metrics_config = get_config()
    for pod in pod_names:
        for metric in metric_queries:
            query = metrics_config[metric]['query'].format(pod=pod)
            results = query_prometheus(query, start_time, end_time, url, step)
            # logging.info(f"{pod} {metric}")
            if results:
                timestamps = [pd.Timestamp.fromtimestamp(int(value[0])) for value in results[0]['values']]
                measures = [float(value[1]) for value in results[0]['values']]
                metric_df = pd.DataFrame({
                    'timestamp': timestamps,
                    f"{pod}_{metric}": measures
                })
                master_df = pd.merge(master_df, metric_df, on='timestamp', how='left')
                # logging.info("\tQuery successful, data merged.")

    # Fill missing data
    master_df.ffill(inplace=True)
    master_df.bfill(inplace=True)
    master_df.fillna(0, inplace=True)
    master_df = master_df.drop('timestamp', axis=1)
    return master_df


def query_prometheus(query, start_time, end_time, url, step=5):
    url = f"{url}/api/v1/query_range"
    params = {'query': query, 'start': start_time, 'end': end_time, 'step': f'{step}s'}
    try:
        response = requests.get(url, params=params)
        response.raise_for_status()
        results = response.json()['data']['result']
        return results
    except requests.exceptions.HTTPError as err:
        logging.error(f"HTTP error occurred: {err}")
    except Exception as err:
        logging.error(f"Other error occurred: {err}")
```
```data_ingestion/utils.py
from kubernetes import client, config as kube_config


def load_kube_config(kube_config_path):
    kube_config.load_kube_config(kube_config_path)


def list_pod_names(namespace):
    v1 = client.CoreV1Api()
    ret = v1.list_namespaced_pod(namespace, watch=False)
    pod_names = [item.metadata.name for item in ret.items]
    return pod_names
```
```data_ingestion/requirements.txt
amqp==5.2.0
billiard==4.2.0
blinker==1.8.2
cachetools==5.3.3
celery==5.4.0
certifi==2024.2.2
charset-normalizer==3.3.2
click==8.1.7
click-didyoumean==0.3.1
click-plugins==1.1.1
click-repl==0.3.0
Flask==3.0.3
Flask-Cors==4.0.1
google-auth==2.29.0
idna==3.7
itsdangerous==2.2.0
Jinja2==3.1.4
joblib==1.4.2
kombu==5.3.7
kubernetes==29.0.0
MarkupSafe==2.1.5
numpy==1.26.4
oauthlib==3.2.2
pandas==2.2.2
prompt_toolkit==3.0.47
pyasn1==0.6.0
pyasn1_modules==0.4.0
python-dateutil==2.9.0.post0
pytz==2024.1
PyYAML==6.0.1
requests==2.31.0
requests-oauthlib==2.0.0
rsa==4.9
scikit-learn==1.4.2
scipy==1.13.0
six==1.16.0
threadpoolctl==3.5.0
tzdata==2024.1
urllib3==2.2.1
vine==5.1.0
wcwidth==0.2.13
websocket-client==1.8.0
Werkzeug==3.0.3
```
```data_processing/app.py
# app.py
from flask import Flask, request, jsonify
import logging
import json
import traceback

from cgnn_preprocess import handle_cgnn_request, handle_cgnn_train_request
from crca_preprocess import handle_crca_request

app = Flask(__name__)
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@app.route('/preprocess_cgnn_train_data', methods=['POST'])
def preprocess_cgnn_train_data():
    try:
        train_data = request.files
        train_info_json = request.form.get('train_info')
        train_info = json.loads(train_info_json)
        response = handle_cgnn_train_request(train_data, train_info)
        return response
    except Exception as e:
        logger.error(traceback.format_exc())
        return jsonify({'error': str(e)}), 500


@app.route('/preprocess_cgnn_data', methods=['POST'])
def preprocess_cgnn_data():
    try:
        test_data = request.files['test_array']
        test_info_json = request.form.get('test_info')
        test_info = json.loads(test_info_json)
        response = handle_cgnn_request(test_data, test_info)
        return response
    except Exception as e:
        logger.error(traceback.format_exc())
        return jsonify({'error': str(e)}), 500


@app.route('/preprocess_crca_data', methods=['POST'])
def preprocess_crca_data():
    try:
        crca_file = request.files['crca_file']
        crca_info_json = request.form.get('crca_info')
        crca_info = json.loads(crca_info_json)
        response = handle_crca_request(crca_file, crca_info)
        return response
    except Exception as e:
        logger.error(traceback.format_exc())
        return jsonify({'error': str(e)}), 500


if __name__ == '__main__':
    app.run(debug=True, host='0.0.0.0', port=5002)
```
```data_processing/cgnn_preprocess.py
import numpy as np
import pandas as pd
import requests
import json
from flask import Response

from ast import literal_eval
from sklearn.preprocessing import MinMaxScaler


def handle_cgnn_request(test_data, test_info):
    test_array, _, _ = load_data(test_data)
    test_data_processed = get_data(test_array, test_array.shape[1])
    test_files = {'test_array': test_data_processed}
    test_info_json = json.dumps(test_info)
    response = requests.post(f"{test_info['settings']['API_CGNN_ANOMALY_DETECTION_URL']}/detect_anomalies",
                             files=test_files, data={'test_info': test_info_json})

    flask_response = Response(
        response.content,
        status=response.status_code,
        content_type=response.headers['Content-Type']
    )
    return flask_response


def handle_cgnn_train_request(cgnn_data, train_info):
    test_array, train_array, anomaly_label_array = load_data(cgnn_data['test_array'],
                                                             cgnn_data['train_array'],
                                                             cgnn_data['anomaly_label_array'],
                                                             train_info['data']['anomaly_sequence'])
    train_data, test_data, anomaly_data = get_data(test_array, test_array.shape[1],
                                                   train_array, anomaly_label_array)
    train_files = {
        'train_array': train_data,
        'test_array': test_data,
        'anomaly_label_array': anomaly_data
    }
    train_info_json = json.dumps(train_info)
    response = requests.post(f"{train_info['settings']['API_LEARNING_ADAPTATION_URL']}/cgnn_train_model",
                             files=train_files, data={'train_info': train_info_json})

    flask_response = Response(
        response.content,
        status=response.status_code,
        content_type=response.headers['Content-Type']
    )
    return flask_response


def load_data(test_data, train_data=None, anomaly_label=None, anomaly_sequence="False"):
    test_df = pd.read_csv(test_data, header=None)
    test_array = test_df.to_numpy(dtype=np.float32)
    if train_data is not None:
        train_df = pd.read_csv(train_data, header=None)
        train_array = train_df.to_numpy(dtype=np.float32)
    else:
        train_array = None
    if anomaly_label is not None:
        if anomaly_sequence == "True":
            anomalies = literal_eval(anomaly_label, header=None)
            length = len(test_array)
            label = np.zeros([length], dtype=np.float32)
            for anomaly in anomalies:
                label[anomaly[0]: anomaly[1] + 1] = 1.0
            anomaly_label_array = np.asarray(label)
        else:
            anomaly_label_df = pd.read_csv(anomaly_label, header=None)
            anomaly_label_array = anomaly_label_df.to_numpy(dtype=np.float32)
    else:
        anomaly_label_array = None
    return test_array, train_array, anomaly_label_array


def get_data(test_array, data_dim, train_array=None, anomaly_label_array=None):
    test_data = test_array.reshape((-1, data_dim))
    if train_array is not None:
        train_data = train_array.reshape((-1, data_dim))
    if anomaly_label_array is not None:
        anomaly_data = anomaly_label_array.reshape((-1))

    if anomaly_label_array is not None:
        train_data, scaler = normalize_data(train_data, scaler=None)
        test_data, _ = normalize_data(test_data, scaler=scaler)
        print("train set shape: ", train_data.shape)
        print("test set shape: ", test_data.shape)
        print("test set label shape: ", None if anomaly_data is None else anomaly_data.shape)
        return (pd.DataFrame(train_data).to_csv(index=False, header=False),
                pd.DataFrame(test_data).to_csv(index=False, header=False),
                pd.DataFrame(anomaly_data).to_csv(index=False, header=False))
    else:
        test_data, _ = normalize_data(test_data, scaler=None)
        print("test set shape: ", test_data.shape)
        return pd.DataFrame(test_data).to_csv(index=False, header=False)


def normalize_data(data, scaler=None):
    data = np.asarray(data, dtype=np.float32)

    if np.any(sum(np.isnan(data))):
        data = np.nan_to_num(data)

    if scaler is None:
        scaler = MinMaxScaler()
        scaler.fit(data)
    data = scaler.transform(data)
    print("Data normalized")

    return data, scaler
```
```data_processing/crca_preprocess.py
from sklearn.preprocessing import MinMaxScaler
import pandas as pd
import requests
import json
from flask import Response


def handle_crca_request(crca_file, crca_info):
    crca_file_processed = normalize_data(crca_file)
    crca_info_json = json.dumps(crca_info)
    response = requests.post(f"{crca_info['settings']['API_CRCA_ANOMALY_DETECTION_URL']}/crca",
                             files={'crca_file': crca_file_processed}, data={'crca_info': crca_info_json})

    flask_response = Response(
        response.content,
        status=response.status_code,
        content_type=response.headers['Content-Type']
    )

    return flask_response


def normalize_data(csv):
    df = pd.read_csv(csv, header=None)
    scaler = MinMaxScaler()
    df_normalized = scaler.fit_transform(df)
    return pd.DataFrame(df_normalized).to_csv(index=False, header=False)
```
```data_processing/requirements.txt
blinker==1.8.2
certifi==2024.6.2
charset-normalizer==3.3.2
click==8.1.7
Flask==3.0.3
idna==3.7
itsdangerous==2.2.0
Jinja2==3.1.4
joblib==1.4.2
MarkupSafe==2.1.5
numpy==1.26.4
pandas==2.2.2
python-dateutil==2.9.0.post0
pytz==2024.1
requests==2.32.3
scikit-learn==1.5.0
scipy==1.13.1
six==1.16.0
threadpoolctl==3.5.0
tzdata==2024.1
urllib3==2.2.1
Werkzeug==3.0.3
```

```anomaly_detection/cgnn/app.py
# app.py
from flask import Flask, request, jsonify
import logging
import torch
import os
import shutil
import json
import pandas as pd
import numpy as np
import traceback
import requests
from collections import OrderedDict
from flask_cors import CORS

from config import set_config, get_config, set_initial_config
from predict import load_model_and_predict

app = Flask(__name__)
CORS(app, resources={r"/*": {"origins": "*"}})

# Ensure logging is set up
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@app.route('/detect_anomalies', methods=['POST'])
def detect_anomalies():
    """
    Detect anomalies in the given data.
    """
    try:
        test_data = pd.read_csv(request.files['test_array'], header=None).to_numpy(dtype=np.float32)
        test_info_json = request.form.get('test_info')
        test_info = json.loads(test_info_json)
        model = test_info['data']['model']
        anomaly_result = load_model_and_predict(test_data, model)
        crca_threshold = test_info['data']['crca_threshold']
        iteration = test_info['data']['iteration']
        task_id = test_info['task_id']

        path = 'results/'+task_id
        if not os.path.isdir(path):
            os.mkdir(path)

        if os.path.exists(path+'/cgnn_results.json'):
            with open(path+'/cgnn_results.json', 'r') as f:
                data = json.load(f)
            data['results'][iteration] = {
                'start_time': test_info['data']['start_time'],
                'end_time': test_info['data']['end_time'],
                'percentage': anomaly_result
            }
        else:
            data = {}
            data['model'] = model
            data['start_time'] = test_info['data']['start_time']
            data['containers'] = test_info['data']['containers']
            data['metrics'] = test_info['data']['metrics']
            data['step'] = test_info['data']['data_interval']
            data['crca_threshold'] = crca_threshold
            data['results'] = {
                iteration: {
                    'start_time': test_info['data']['start_time'],
                    'end_time': test_info['data']['end_time'],
                    'percentage': anomaly_result
                }
            }

        if anomaly_result > crca_threshold:
            crca_data = {
                'settings': test_info['settings'],
                'data': {
                    'task_id': task_id,
                    'start_time': test_info['data']['start_time'],
                    'end_time': test_info['data']['end_time'],
                    'containers': test_info['data']['crca_pods'],
                    'metrics': test_info['data']['metrics'],
                    'step': test_info['data']['data_interval']
                }
            }
            crca_data_json = json.dumps(crca_data)

            response = requests.post(f"{test_info['settings']['API_DATA_INGESTION_URL']}/anomaly_rca",
                                     data={'crca_data': crca_data_json})
            task_data = response.json()
            task_id = task_data.get('task_id')
            if task_id:
                data['results'][iteration]['crca_task_id'] = task_id

        with open(path+'/cgnn_results.json', 'w') as f:
            json.dump(data, f, indent=2)

        return "success"
    except Exception as e:
        logger.error(traceback.format_exc())
        return jsonify({"error": str(e)}), 500


@app.route('/save_model', methods=['POST'])
def save_model():
    try:
        trained_model = request.files['model']
        model_info_json = request.form.get('model_info')
        model_info = json.loads(model_info_json)
        model = next(iter(model_info['data']))

        model_path = 'trained_models/' + model

        if not os.path.isdir(model_path):
            os.mkdirs(model_path)
        model_json = trained_model.read().decode('utf-8')
        model_dict = json.loads(model_json)
        loaded_model = OrderedDict({key: torch.tensor(value) for key, value in model_dict.items()})
        torch.save(loaded_model, model_path+'/model.pt')
        with open(model_path+'/model_params.json', 'w') as f:
            json.dump(model_info['data'][model]['model_params'], f, indent=2)
        with open(model_path+'/model_config.json', 'w') as f:
            json.dump(model_info['data'][model]['model_config'], f, indent=2)
        with open(model_path+'/model_evaluation.json', 'w') as f:
            json.dump(model_info['data'][model]['model_evaluation'], f, indent=2)

        return jsonify("success"), 200
    except Exception as e:
        logger.error(traceback.format_exc())
        return jsonify({"error": str(e)}), 500


@app.route('/get_config', methods=['GET'])
def get_config_route():
    try:
        config = get_config()
        return jsonify(config), 200
    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route('/update_config', methods=['POST'])
def update_config():
    """
    Updates the configuration for the monitoring application.
    """
    new_config = request.json
    try:
        # Assuming set_config is a function that updates your configurations
        set_config(new_config)
        return jsonify({"success"}), 200
    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route('/get_available_models', methods=['GET'])
def get_available_models():
    try:
        model_dir = 'trained_models/'
        models = {}
        for model in os.listdir(model_dir):
            model_path = os.path.join(model_dir, model)
            if os.path.isdir(model_path):
                models[model] = {}
                with open(model_path+'/model_params.json', 'r') as f:
                    models[model]['model_params'] = json.load(f)
                with open(model_path+'/model_config.json', 'r') as f:
                    models[model]['model_config'] = json.load(f)
                with open(model_path+'/model_evaluation.json', 'r') as f:
                    models[model]['model_evaluation'] = json.load(f)
        return jsonify(models), 200
    except Exception as e:
        logger.error(traceback.format_exc())
        return jsonify({"error": str(e)}), 500


@app.route('/get_results', methods=['POST'])
def get_results():
    try:
        task_id = request.json['task_id']
        with open('results/'+task_id+'/cgnn_results.json', 'r') as f:
            data = json.load(f)
        return jsonify(data), 200
    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route('/get_all_results', methods=['GET'])
def get_all_results():
    try:
        task_ids = os.listdir('results/')
        results = {}
        for task_id in task_ids:
            result_file = 'results/'+task_id+'/cgnn_results.json'
            if os.path.isfile(result_file):
                with open('results/'+task_id+'/cgnn_results.json', 'r') as f:
                    data = json.load(f)
                results[task_id] = data
        return jsonify(results), 200
    except Exception as e:
        logger.error(traceback.format_exc())
        return jsonify({"error": str(e)}), 500


@app.route('/delete_results', methods=['POST'])
def delete_results():
    try:
        data = request.get_json()
        task_id = data['taskId']
        crca_link = data['crcaLink']
        with open('results/'+task_id+'/cgnn_results.json', 'r') as f:
            data = json.load(f)
        print(data['results'], task_id, crca_link)
        for key, result in data['results'].items():
            if 'crca_task_id' in result:
                requests.delete(f"{crca_link}/delete_results/{result['crca_task_id']}")
        shutil.rmtree('results/'+task_id)
        return jsonify({"success": True}), 200
    except Exception as e:
        logger.error(traceback.format_exc())
        return jsonify({"error": str(e)}), 500


if __name__ == '__main__':
    set_initial_config()
    app.run(debug=True, host='0.0.0.0', port=5013)

```
```anomaly_detection/cgnn/config.py
# config.py
import os
import json

config_file_path = "config.json"


def set_initial_config():
    config = {
        # -- Data params ---
        "dataset": os.getenv("DATASET", ""),
        "group": os.getenv("GROUP", ""),
        "data_dim": os.getenv("DATA_DIM", ),
        "lookback": os.getenv("LOOKBACK", 100),
        # "normalize": os.getenv("NORMALIZE", True),
        "spec_res": os.getenv("SPEC_RES", False),
        # -- Model params ---
        # 1D conv layer
        "kernel_size": os.getenv("KERNEL_SIZE", 7),
        # GAT layers
        "use_gatv2": os.getenv("USE_GATV2", True),
        "feat_gat_embed_dim": os.getenv("FEAT_GAT_EMBED_DIM", None),
        "time_gat_embed_dim": os.getenv("TIME_GAT_EMBED_DIM", None),
        # GRU layer
        "gru_n_layers": os.getenv("GRU_N_LAYERS", 1),
        "gru_hid_dim": os.getenv("GRU_HID_DIM", 150),
        # Forecasting Model
        "fc_n_layers": os.getenv("FC_N_LAYERS", 3),
        "fc_hid_dim": os.getenv("FC_HID_DIM", 150),
        # Other
        "alpha": os.getenv("ALPHA", 0.2),
        # --- Train params ---
        "epochs": os.getenv("EPOCHS", 10),
        "val_split": os.getenv("VAL_SPLIT", 0.1),
        "bs": os.getenv("BS", 256),
        "init_lr": os.getenv("INIT_LR", 1e-3),
        "shuffle_dataset": os.getenv("SHUFFLE_DATASET", True),
        "dropout": os.getenv("DROPOUT", 0.4),
        "use_cuda": os.getenv("USE_CUDA", True),
        "print_every": os.getenv("PRINT_EVERY", 1),
        "log_tensorboard": os.getenv("LOG_TENSORBOARD", True),
        # --- Predictor params ---
        "scale_scores": os.getenv("SCALE_SCORES", False),
        "use_mov_av": os.getenv("USE_MOV_AV", False),
        "gamma": os.getenv("GAMMA", 1),
        "level": os.getenv("LEVEL", 0.90),
        "q": os.getenv("Q", 0.001),
        "reg_level": os.getenv("REG_LEVEL", 1),
        "dynamic_pot": os.getenv("DYNAMIC_POT", False),
        # --- Other ---
        "comment": os.getenv("COMMENT", "")
    }
    with open(config_file_path, 'w') as file:
        json.dump(config, file, indent=2)


def get_config():
    with open(config_file_path, 'r') as file:
        config = json.load(file)
    return config


def set_config(new_config=None):
    config = get_config()
    if new_config:
        config.update(new_config)
    with open(config_file_path, 'w') as file:
        json.dump(config, file, indent=2)

```
```anomaly_detection/cgnn/modules.py
import torch
import torch.nn as nn
import numpy as np


class ConvLayer(nn.Module):
    """1-D Convolution layer to extract high-level features of each time-series input
    :param n_features: Number of input features/nodes
    :param window_size: length of the input sequence
    :param kernel_size: size of kernel to use in the convolution operation
    """

    def __init__(self, n_features, kernel_size=7):
        super(ConvLayer, self).__init__()
        self.padding = nn.ConstantPad1d((kernel_size - 1) // 2, 0.0)
        self.conv = nn.Conv1d(in_channels=n_features, out_channels=n_features, kernel_size=kernel_size)
        self.relu = nn.ReLU()

    def forward(self, x):
        x = x.permute(0, 2, 1)
        x = self.padding(x)
        x = self.relu(self.conv(x))
        return x.permute(0, 2, 1)  # Permute back


def TemporalcorrelationLayer(x):
    use_cuda = True  #
    device = "cuda" if use_cuda and torch.cuda.is_available() else "cpu"
    matrix_all = []
    y = x.data.cpu().numpy()

    for k in range(y.shape[0]):
        data = y[k]
        matrix = np.zeros((data.shape[0], data.shape[0]))
        for i in range(data.shape[0]):
            for j in range(data.shape[1]):
                matrix[i][j] = np.correlate(data[i, :], data[j, :])

        matrix = matrix / data.shape[0]
        matrix_all.append(matrix)
    attention = torch.from_numpy(np.array(matrix_all))
    attention = attention.to(dtype=torch.float32)

    attention = attention.to(device)
    h = torch.sigmoid(torch.matmul(attention, x))  # (b, n, k)

    return h


def FeaturecorrelationLayer(x):
    # print(f'x={x.shape}')
    use_cuda = True  #
    device = "cuda" if use_cuda and torch.cuda.is_available() else "cpu"
    matrix_all = []
    y = x.data.cpu().numpy()

    for k in range(y.shape[0]):
        data = y[k]
        matrix = np.zeros((data.shape[1], data.shape[1]))
        for i in range(data.shape[0]):
            for j in range(data.shape[1]):
                if (i <= j):
                    matrix[i][j] = np.inner(data[:, i], data[:, j])
                else:
                    break
        matrix = matrix / data.shape[0]
        matrix_all.append(matrix)
    attention = torch.from_numpy(np.array(matrix_all))
    attention = attention.to(dtype=torch.float32)
    attention = attention.to(device)
    # print(attention.shape)
    h = torch.sigmoid(torch.matmul(attention, x.permute(0, 2, 1)))
    # print(f'h={h.shape}')
    return h.permute(0, 2, 1)


class GRULayer(nn.Module):
    """Gated Recurrent Unit (GRU) Layer
    :param in_dim: number of input features
    :param hid_dim: hidden size of the GRU
    :param n_layers: number of layers in GRU
    :param dropout: dropout rate
    """

    def __init__(self, in_dim, hid_dim, n_layers, dropout):
        super(GRULayer, self).__init__()
        self.hid_dim = hid_dim
        self.n_layers = n_layers
        self.dropout = 0.0 if n_layers == 1 else dropout
        self.gru = nn.GRU(in_dim, hid_dim, num_layers=n_layers, batch_first=True, dropout=self.dropout)

    def forward(self, x):
        out, h = self.gru(x)
        out, h = out[-1, :, :], h[-1, :, :]  # Extracting from last layer
        return out, h


class Forecasting_Model(nn.Module):
    """Forecasting model (fully-connected network)
    :param in_dim: number of input features
    :param hid_dim: hidden size of the FC network
    :param out_dim: number of output features
    :param n_layers: number of FC layers
    :param dropout: dropout rate
    """

    def __init__(self, in_dim, hid_dim, out_dim, n_layers, dropout):
        super(Forecasting_Model, self).__init__()
        layers = [nn.Linear(in_dim, hid_dim)]

        for _ in range(n_layers - 1):
            layers.append(nn.Linear(hid_dim, hid_dim))

        layers.append(nn.Linear(hid_dim, out_dim))

        self.layers = nn.ModuleList(layers)
        self.dropout = nn.Dropout(dropout)
        self.relu = nn.ReLU()

    def forward(self, x):
        for i in range(len(self.layers) - 1):
            x = self.relu(self.layers[i](x))
            x = self.dropout(x)

        return self.layers[-1](x)


def Denoising(train):
    use_cuda = True
    device = "cuda" if use_cuda and torch.cuda.is_available() else "cpu"

    io_all = []
    for i in range(train.shape[0]):
        data = train[i]
        data = data.data.cpu().numpy()
        io_time = []
        for j in range(data.shape[1]):
            x = data[:, j]
            # x = x.data.cpu().numpy()
            f = np.fft.rfft(x)
            yf_abs = np.abs(f)
            indices = yf_abs > yf_abs.mean()  # filter out those value under 300
            yf_clean = indices * f
            new_f_clean = np.fft.irfft(yf_clean)
            io_time.append(new_f_clean)
        io_time = np.array(io_time)
        io_all.append(io_time)
    io_all = np.array(io_all)
    io_all = torch.from_numpy(np.array(io_all))
    io_all = io_all.to(dtype=torch.float32)
    io_all = io_all.permute(0, 2, 1)
    io_all = io_all.to(device)
    return io_all


class AR(nn.Module):

    def __init__(self, window):
        super(AR, self).__init__()
        self.linear = nn.Linear(window, 1)

    def forward(self, x):
        x = torch.transpose(x, 1, 2)
        x = self.linear(x)
        x = torch.transpose(x, 1, 2)

        return x


class MHSA(nn.Module):
    def __init__(self, num_heads, dim):
        super().__init__()

        # Q, K, V 转换矩阵，这里假设输入和输出的特征维度相同
        self.q = nn.Linear(dim, dim)
        self.k = nn.Linear(dim, dim)
        self.v = nn.Linear(dim, dim)
        self.num_heads = num_heads

    def forward(self, x):
        # print(x.shape)
        B, N, C = x.shape
        # 生成转换矩阵并分多头
        q = self.q(x).reshape(B, N, self.num_heads, -1).permute(0, 2, 1, 3)
        k = self.k(x).reshape(B, N, self.num_heads, -1).permute(0, 2, 1, 3)
        v = self.k(x).reshape(B, N, self.num_heads, -1).permute(0, 2, 1, 3)

        # 点积得到attention score
        attn = q @ k.transpose(2, 3) * (x.shape[-1] ** -0.5)
        attn = attn.softmax(dim=-1)
        # 乘上attention score并输出
        v = (attn @ v).permute(0, 2, 1, 3).reshape(B, N, C)
        # print(v.shape)
        return v
```
```anomaly_detection/cgnn/mtad_gat.py
import torch
import torch.nn as nn
from modules import (
    Denoising,
    ConvLayer,
    GRULayer,
    Forecasting_Model,
    MHSA,
    AR,
    TemporalcorrelationLayer,
    FeaturecorrelationLayer,
)


class MTAD_GAT(nn.Module):
    """ MTAD-GAT model class.

    :param n_features: Number of input features
    :param window_size: Length of the input sequence
    :param out_dim: Number of features to output
    :param kernel_size: size of kernel to use in the 1-D convolution
    :param feat_gat_embed_dim: embedding dimension (output dimension of linear transformation)
           in feat-oriented GAT layer
    :param time_gat_embed_dim: embedding dimension (output dimension of linear transformation)
           in time-oriented GAT layer
    :param use_gatv2: whether to use the modified attention mechanism of GATv2 instead of standard GAT
    :param gru_n_layers: number of layers in the GRU layer
    :param gru_hid_dim: hidden dimension in the GRU layer
    :param forecast_n_layers: number of layers in the FC-based Forecasting Model
    :param forecast_hid_dim: hidden dimension in the FC-based Forecasting Model
    :param recon_n_layers: number of layers in the GRU-based Reconstruction Model
    :param recon_hid_dim: hidden dimension in the GRU-based Reconstruction Model
    :param dropout: dropout rate
    :param alpha: negative slope used in the leaky rely activation function
        d_k (int): d_model / n_head
        d_v (int): d_model / n_head
        d_model (int): outputs of dimension
        n_head (int): num of Multi-head
    """

    def __init__(
        self,
        n_features,
        window_size,
        out_dim,
        kernel_size=7,
        feat_gat_embed_dim=None,
        time_gat_embed_dim=None,
        use_gatv2=True,
        gru_n_layers=1,
        gru_hid_dim=150,
        forecast_n_layers=1,
        forecast_hid_dim=150,
        dropout=0.4,
        alpha=0.2,

    ):
        super(MTAD_GAT, self).__init__()

        self.conv = ConvLayer(n_features, kernel_size)
        #
        # self.feature_gat = FeatureAttentionLayer(n_features, window_size, dropout, alpha, feat_gat_embed_dim, use_gatv2)
        # self.temporal_gat = TemporalAttentionLayer(n_features, window_size, dropout, alpha, time_gat_embed_dim, use_gatv2)

        self.multiheadattention = MHSA(n_features, 3*n_features)
        self.gru = GRULayer(3 * n_features, gru_hid_dim, gru_n_layers, dropout)

        self.ar = AR(window_size)
        self.forecasting_model = Forecasting_Model(gru_hid_dim, forecast_hid_dim, out_dim, forecast_n_layers, dropout)
        # self.forecasting_model = Forecasting_Model(3*n_features*window_size, forecast_hid_dim, out_dim, forecast_n_layers, dropout)
        # self.linear=nn.Linear(n_features,out_dim)

    def forward(self, x):
        # x shape (b, n, k): b - batch size, n - window size, k - number of features
        gamma = 0.5
        x = Denoising(x)
        h_a = self.ar(x)
        h_a = h_a.view(x.shape[0], -1)

        x = self.conv(x)
        # h_feat = self.feature_gat(x)
        # h_temp = self.temporal_gat(x)
        h_feat = FeaturecorrelationLayer(x)
        h_temp = TemporalcorrelationLayer(x)
        # h_cat = torch.cat([x,  h_temp], dim=2)
        h_cat = torch.cat([x, h_feat, h_temp], dim=2)  # (b, n, 3k)   (256,100,38*3)
        # print(h_cat.shape)
        h_in = self.multiheadattention(h_cat)
        # print(h_in.shape)
        # h_in=self.mhsa(h_cat)
        # print(h_in.shape)
        # print(h_in_r.shape)
        # h_in = h_in.view(x.shape[0], -1)   #
        # print(h_in.shape)
        _, h_end = self.gru(h_in)

        h_end = h_end.view(x.shape[0], -1)   # Hidden state for last timestamp
        predictions = self.forecasting_model(h_end)

        predictions_a = gamma*predictions+(1-gamma)*h_a
        return predictions_a

```
```anomaly_detection/cgnn/predict.py
import json
import os
import torch

from config import set_config, get_config, set_initial_config
from utils import load, create_data_loaders, SlidingWindowDataset
from mtad_gat import MTAD_GAT
from prediction import Predictor


def load_model_and_predict(test_array, dataset, save_output=True):
    if os.path.isdir(f"trained_models/{dataset}"):
        model_path = f"trained_models/{dataset}"
    else:
        raise Exception(f'Dataset "{dataset}" not available.')

    # Check that model exist
    if not os.path.isfile(f"{model_path}/model.pt"):
        raise Exception(f"<{model_path}/model.pt> does not exist.")

    model_config_path = f"{model_path}/model_config.json"

    # Get configs of model
    print(f'Using model from {model_path}')
    set_initial_config()
    with open(model_config_path, "r") as f:
        set_config(json.load(f))
    model_config = get_config()
    print(f'Model config: {model_config}')

    window_size = model_config['lookback']
    batch_size = model_config['bs']
    val_split = model_config['val_split']
    shuffle_dataset = model_config['shuffle_dataset']

    x_test = torch.from_numpy(test_array).float()
    n_features = x_test.shape[1]
    target_dims = None
    # TODO check if this is correct
    # TODO
    # TODO CHEKCHECKECHEKCHEKCHECK
    out_dim = 38

    test_dataset = SlidingWindowDataset(x_test, window_size, target_dims)
    train_loader, val_loader, test_loader = create_data_loaders(
        test_dataset, batch_size, val_split, shuffle_dataset
    )
    test_dataset = SlidingWindowDataset(x_test, window_size, target_dims)

    model = MTAD_GAT(
        n_features,
        window_size,
        out_dim,
        kernel_size=model_config['kernel_size'],
        use_gatv2=model_config['use_gatv2'],
        feat_gat_embed_dim=model_config['feat_gat_embed_dim'],
        time_gat_embed_dim=model_config['time_gat_embed_dim'],
        gru_n_layers=model_config['gru_n_layers'],
        gru_hid_dim=model_config['gru_hid_dim'],
        forecast_n_layers=model_config['fc_n_layers'],
        forecast_hid_dim=model_config['fc_hid_dim'],
        dropout=model_config['dropout'],
        alpha=model_config['alpha']
    )

    device = "cuda" if model_config['use_cuda'] and torch.cuda.is_available() else "cpu"
    load(model, f"{model_path}/model.pt", device=device)
    model.to(device)

    # # Some suggestions for POT args
    # level_q_dict = {
    #     "SMAP": (0.90, 0.005),
    #     "MSL": (0.90, 0.001),
    #     "SMD-1": (0.9950, 0.001),
    #     "SMD-2": (0.9925, 0.001),
    #     "SMD-3": (0.9999, 0.001)
    # }
    # key = "SMD-" + model_config['group'][0] if model_config['dataset'] == "SMD" else model_config['dataset']
    # if key in level_q_dict:
    #     level, q = level_q_dict[key]
    # else:
    #     level, q = (0.9950, 0.001)
    # if model_config['level'] is not None:
    #     level = model_config['level']
    # if model_config['q'] is not None:
    #     q = model_config['q']

    # # Some suggestions for Epsilon args
    # reg_level_dict = {"SMAP": 0, "MSL": 0, "SMD-1": 1, "SMD-2": 1, "SMD-3": 1}
    # key = "SMD-" + model_config['group'][0] if dataset == "SMD" else dataset
    # if key in reg_level_dict:
    #     reg_level = reg_level_dict[key]
    # else:
    #     reg_level = 1
    level = model_config['level']
    q = model_config['q']
    reg_level = model_config['reg_level']

    prediction_args = {
        'dataset': dataset,
        "target_dims": target_dims,
        'scale_scores': model_config['scale_scores'],
        "level": level,
        "q": q,
        'dynamic_pot': model_config['dynamic_pot'],
        "use_mov_av": model_config['use_mov_av'],
        "gamma": model_config['gamma'],
        "reg_level": reg_level,
        "save_path": f"{model_path}",
    }

    # load summary.txt file a json dict
    summary_file_name = f"{model_path}/model_evaluation.json"
    if os.path.isfile(summary_file_name):
        with open(summary_file_name, "r") as f:
            summary = json.load(f)
            prediction_args.update(summary)
        global_epsilon = summary['epsilon_result']['threshold']
    else:
        global_epsilon = 0.1
    print(global_epsilon)

    predictor = Predictor(model, window_size, n_features, prediction_args)
    df_test = predictor.predict_anomalies_without_labels(x_test, global_epsilon, save_output=save_output)

    return check_anomalies(df_test)


def check_anomalies(df):
    anomalies = df['A_Pred_Global'].gt(0).sum()
    total = len(df)
    percentage = anomalies / total * 100
    return percentage
```
```anomaly_detection/cgnn/prediction.py
import numpy as np
import pandas as pd
from tqdm import tqdm
import torch

from utils import SlidingWindowDataset, adjust_anomaly_scores


class Predictor:
    """MTAD-GAT predictor class.

    :param model: MTAD-GAT model (pre-trained) used to forecast and reconstruct
    :param window_size: Length of the input sequence
    :param n_features: Number of input features
    :param pred_args: params for thresholding and predicting anomalies

    """

    def __init__(self, model, window_size, n_features, pred_args, summary_file_name="model_evaluation"):
        self.model = model
        self.window_size = window_size
        self.n_features = n_features
        self.dataset = pred_args["dataset"]
        self.target_dims = pred_args["target_dims"]
        self.scale_scores = pred_args["scale_scores"]
        self.q = pred_args["q"]
        self.level = pred_args["level"]
        self.dynamic_pot = pred_args["dynamic_pot"]
        self.use_mov_av = pred_args["use_mov_av"]
        self.gamma = pred_args["gamma"]
        self.reg_level = pred_args["reg_level"]
        self.save_path = pred_args["save_path"]
        self.batch_size = 256
        self.use_cuda = True  #
        self.pred_args = pred_args
        self.summary_file_name = summary_file_name

    def get_score(self, values):
        """Method that calculates anomaly score using given model and data
        :param values: 2D array of multivariate time series data, shape (N, k)
        :return np array of anomaly scores + dataframe with prediction for each channel and global anomalies
        """

        print("Predicting and calculating anomaly scores..")
        data = SlidingWindowDataset(values, self.window_size, self.target_dims)
        loader = torch.utils.data.DataLoader(data, batch_size=self.batch_size, shuffle=False)
        device = "cuda" if self.use_cuda and torch.cuda.is_available() else "cpu"

        self.model.eval()
        preds = []
        with torch.no_grad():
            for x, y in tqdm(loader):
                x = x.to(device)
                y = y.to(device)

                y_hat = self.model(x)

                preds.append(y_hat.detach().cpu().numpy())

        preds = np.concatenate(preds, axis=0)
        actual = values.detach().cpu().numpy()[self.window_size:]

        if self.target_dims is not None:
            actual = actual[:, self.target_dims]

        anomaly_scores = np.zeros_like(actual)
        df = pd.DataFrame()
        for i in range(preds.shape[1]):
            df[f"Forecast_{i}"] = preds[:, i]
            df[f"True_{i}"] = actual[:, i]
            a_score = np.sqrt((preds[:, i] - actual[:, i]) ** 2)

            if self.scale_scores:
                q75, q25 = np.percentile(a_score, [75, 25])
                iqr = q75 - q25
                median = np.median(a_score)
                a_score = (a_score - median) / (1+iqr)

            anomaly_scores[:, i] = a_score
            df[f"A_Score_{i}"] = a_score
        # print(anomaly_scores.shape)
        score = anomaly_scores
        anomaly_scores = np.mean(anomaly_scores, 1)
        df['A_Score_Global'] = anomaly_scores

        return df, score

    def predict_anomalies_without_labels(self, test, global_epsilon, save_output=True):
        """ Predicts anomalies

        :param test: 2D array of test multivariate time series data
        :param load_scores: Whether to load anomaly scores instead of calculating them
        :param save_output: Whether to save output dataframe
        """

        test_pred_df, test_score = self.get_score(test)
        test_anomaly_scores = test_pred_df['A_Score_Global'].values
        # print("voor adjust", test_anomaly_scores)
        # test_anomaly_scores = adjust_anomaly_scores(test_anomaly_scores, self.dataset, True, self.window_size)
        # print("na adjust", test_anomaly_scores)
        test_pred_df['A_Score_Global'] = test_anomaly_scores

        if self.use_mov_av:
            smoothing_window = int(self.batch_size * self.window_size * 0.05)
            test_anomaly_scores = pd.DataFrame(test_anomaly_scores).ewm(span=smoothing_window).mean().values.flatten()

        test_preds_global = (test_anomaly_scores >= global_epsilon).astype(int)
        test_pred_df["A_Pred_Global"] = test_preds_global
        print(test_pred_df)
        # Save anomaly predictions made using epsilon method (could be changed to pot or bf-method)
        # if save_output:
        #     print(f"Saving output to {self.save_path}/<train/test>_output.pkl")
        #     test_pred_df.to_pickle(f"{self.save_path}/test_output.pkl")

        print("-- Done.")
        return test_pred_df

```
```anomaly_detection/cgnn/spot.py
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Mon Dec 12 10:08:16 2016
@author: Alban Siffer
@company: Amossys
@license: GNU GPLv3
Code from https://github.com/NetManAIOps/OmniAnomaly
"""
from math import floor, log

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import tqdm
from scipy.optimize import minimize

# colors for plot
deep_saffron = "#FF9933"
air_force_blue = "#5D8AA8"

"""
================================= MAIN CLASS ==================================
"""


class SPOT:
    """
    This class allows to run SPOT algorithm on univariate dataset (upper-bound)

    Attributes
    ----------
    proba : float
            Detection level (risk), chosen by the user

    extreme_quantile : float
            current threshold (bound between normal and abnormal events)

    data : numpy.array
            stream

    init_data : numpy.array
            initial batch of observations (for the calibration/initialization step)

    init_threshold : float
            initial threshold computed during the calibration step

    peaks : numpy.array
            array of peaks (excesses above the initial threshold)

    n : int
            number of observed values

    Nt : int
            number of observed peaks
    """

    def __init__(self, q=1e-4):
        """
        Constructor
        Parameters
        ----------
        q
                Detection level (risk)

        Returns
        ----------
        SPOT object
        """
        self.proba = q
        self.extreme_quantile = None
        self.data = None
        self.init_data = None
        self.init_threshold = None
        self.peaks = None
        self.n = 0
        self.Nt = 0

    def __str__(self):
        s = ""
        s += "Streaming Peaks-Over-Threshold Object\n"
        s += "Detection level q = %s\n" % self.proba
        if self.data is not None:
            s += "Data imported : Yes\n"
            s += "\t initialization  : %s values\n" % self.init_data.size
            s += "\t stream : %s values\n" % self.data.size
        else:
            s += "Data imported : No\n"
            return s

        if self.n == 0:
            s += "Algorithm initialized : No\n"
        else:
            s += "Algorithm initialized : Yes\n"
            s += "\t initial threshold : %s\n" % self.init_threshold

            r = self.n - self.init_data.size
            if r > 0:
                s += "Algorithm run : Yes\n"
                s += "\t number of observations : %s (%.2f %%)\n" % (
                    r,
                    100 * r / self.n,
                )
            else:
                s += "\t number of peaks  : %s\n" % self.Nt
                s += "\t extreme quantile : %s\n" % self.extreme_quantile
                s += "Algorithm run : No\n"
        return s

    def fit(self, init_data, data):
        """
        Import data to SPOT object

        Parameters
        ----------
        init_data : list, numpy.array or pandas.Series
                initial batch to calibrate the algorithm

        data : numpy.array
                data for the run (list, np.array or pd.series)

        """
        if isinstance(data, list):
            self.data = np.array(data)
        elif isinstance(data, np.ndarray):
            self.data = data
        elif isinstance(data, pd.Series):
            self.data = data.values
        else:
            print("This data format (%s) is not supported" % type(data))
            return

        if isinstance(init_data, list):
            self.init_data = np.array(init_data)
        elif isinstance(init_data, np.ndarray):
            self.init_data = init_data
        elif isinstance(init_data, pd.Series):
            self.init_data = init_data.values
        elif isinstance(init_data, int):
            self.init_data = self.data[:init_data]
            self.data = self.data[init_data:]
        elif isinstance(init_data, float) & (init_data < 1) & (init_data > 0):
            r = int(init_data * data.size)
            self.init_data = self.data[:r]
            self.data = self.data[r:]
        else:
            print("The initial data cannot be set")
            return

    def add(self, data):
        """
        This function allows to append data to the already fitted data

        Parameters
        ----------
        data : list, numpy.array, pandas.Series
                data to append
        """
        if isinstance(data, list):
            data = np.array(data)
        elif isinstance(data, np.ndarray):
            data = data
        elif isinstance(data, pd.Series):
            data = data.values
        else:
            print("This data format (%s) is not supported" % type(data))
            return

        self.data = np.append(self.data, data)
        return

    def initialize(self, level=0.98, min_extrema=False, verbose=True):
        """
        Run the calibration (initialization) step

        Parameters
        ----------
        level : float
                (default 0.98) Probability associated with the initial threshold t
        verbose : bool
                (default = True) If True, gives details about the batch initialization
        verbose: bool
                (default True) If True, prints log
        min_extrema bool
                (default False) If True, find min extrema instead of max extrema
        """
        if min_extrema:
            self.init_data = -self.init_data
            self.data = -self.data
            level = 1 - level

        level = level - floor(level)

        n_init = self.init_data.size

        S = np.sort(self.init_data)  # we sort X to get the empirical quantile
        self.init_threshold = S[int(level * n_init)]  # t is fixed for the whole algorithm

        # initial peaks
        self.peaks = self.init_data[self.init_data > self.init_threshold] - self.init_threshold
        self.Nt = self.peaks.size
        self.n = n_init

        if verbose:
            print("Initial threshold : %s" % self.init_threshold)
            print("Number of peaks : %s" % self.Nt)
            print("Grimshaw maximum log-likelihood estimation ... ", end="")

        g, s, l = self._grimshaw()
        self.extreme_quantile = self._quantile(g, s)

        if verbose:
            print("[done]")
            print("\t" + chr(0x03B3) + " = " + str(g))
            print("\t" + chr(0x03C3) + " = " + str(s))
            print("\tL = " + str(l))
            print("Extreme quantile (probability = %s): %s" % (self.proba, self.extreme_quantile))

        return

    def _rootsFinder(fun, jac, bounds, npoints, method):
        """
        Find possible roots of a scalar function

        Parameters
        ----------
        fun : function
                scalar function
        jac : function
                first order derivative of the function
        bounds : tuple
                (min,max) interval for the roots search
        npoints : int
                maximum number of roots to output
        method : str
                'regular' : regular sample of the search interval, 'random' : uniform (distribution) sample of the search interval

        Returns
        ----------
        numpy.array
                possible roots of the function
        """
        if method == "regular":
            step = (bounds[1] - bounds[0]) / (npoints + 1)
            X0 = np.arange(bounds[0] + step, bounds[1], step)
        elif method == "random":
            X0 = np.random.uniform(bounds[0], bounds[1], npoints)

        def objFun(X, f, jac):
            g = 0
            j = np.zeros(X.shape)
            i = 0
            for x in X:
                fx = f(x)
                g = g + fx ** 2
                j[i] = 2 * fx * jac(x)
                i = i + 1
            return g, j

        opt = minimize(
            lambda X: objFun(X, fun, jac),
            X0,
            method="L-BFGS-B",
            jac=True,
            bounds=[bounds] * len(X0),
        )

        X = opt.x
        np.round(X, decimals=5)
        return np.unique(X)

    def _log_likelihood(Y, gamma, sigma):
        """
        Compute the log-likelihood for the Generalized Pareto Distribution (μ=0)

        Parameters
        ----------
        Y : numpy.array
                observations
        gamma : float
                GPD index parameter
        sigma : float
                GPD scale parameter (>0)
        Returns
        ----------
        float
                log-likelihood of the sample Y to be drawn from a GPD(γ,σ,μ=0)
        """
        n = Y.size
        if gamma != 0:
            tau = gamma / sigma
            L = -n * log(sigma) - (1 + (1 / gamma)) * (np.log(1 + tau * Y)).sum()
        else:
            L = n * (1 + log(Y.mean()))
        return L

    def _grimshaw(self, epsilon=1e-8, n_points=10):
        """
        Compute the GPD parameters estimation with the Grimshaw's trick

        Parameters
        ----------
        epsilon : float
                numerical parameter to perform (default : 1e-8)
        n_points : int
                maximum number of candidates for maximum likelihood (default : 10)
        Returns
        ----------
        gamma_best,sigma_best,ll_best
                gamma estimates, sigma estimates and corresponding log-likelihood
        """

        def u(s):
            return 1 + np.log(s).mean()

        def v(s):
            return np.mean(1 / s)

        def w(Y, t):
            s = 1 + t * Y
            us = u(s)
            vs = v(s)
            return us * vs - 1

        def jac_w(Y, t):
            s = 1 + t * Y
            us = u(s)
            vs = v(s)
            jac_us = (1 / t) * (1 - vs)
            jac_vs = (1 / t) * (-vs + np.mean(1 / s ** 2))
            return us * jac_vs + vs * jac_us

        Ym = self.peaks.min()
        YM = self.peaks.max()
        Ymean = self.peaks.mean()

        a = -1 / YM
        if abs(a) < 2 * epsilon:
            epsilon = abs(a) / n_points

        a = a + epsilon
        b = 2 * (Ymean - Ym) / (Ymean * Ym)
        c = 2 * (Ymean - Ym) / (Ym ** 2)

        # We look for possible roots
        left_zeros = SPOT._rootsFinder(
            lambda t: w(self.peaks, t),
            lambda t: jac_w(self.peaks, t),
            (a + epsilon, -epsilon),
            n_points,
            "regular",
        )

        right_zeros = SPOT._rootsFinder(
            lambda t: w(self.peaks, t),
            lambda t: jac_w(self.peaks, t),
            (b, c),
            n_points,
            "regular",
        )

        # all the possible roots
        zeros = np.concatenate((left_zeros, right_zeros))

        # 0 is always a solution so we initialize with it
        gamma_best = 0
        sigma_best = Ymean
        ll_best = SPOT._log_likelihood(self.peaks, gamma_best, sigma_best)

        # we look for better candidates
        for z in zeros:
            gamma = u(1 + z * self.peaks) - 1
            sigma = gamma / z
            ll = SPOT._log_likelihood(self.peaks, gamma, sigma)
            if ll > ll_best:
                gamma_best = gamma
                sigma_best = sigma
                ll_best = ll

        return gamma_best, sigma_best, ll_best

    def _quantile(self, gamma, sigma):
        """
        Compute the quantile at level 1-q

        Parameters
        ----------
        gamma : float
                GPD parameter
        sigma : float
                GPD parameter
        Returns
        ----------
        float
                quantile at level 1-q for the GPD(γ,σ,μ=0)
        """
        r = self.n * self.proba / self.Nt
        if gamma != 0:
            return self.init_threshold + (sigma / gamma) * (pow(r, -gamma) - 1)
        else:
            return self.init_threshold - sigma * log(r)

    def run(self, with_alarm=True, dynamic=True):
        """
        Run SPOT on the stream

        Parameters
        ----------
        with_alarm : bool
            (default = True) If False, SPOT will adapt the threshold assuming \
            there is no abnormal values
        Returns
        ----------
        dict
            keys : 'thresholds' and 'alarms'

            'thresholds' contains the extreme quantiles and 'alarms' contains \
            the indexes of the values which have triggered alarms

        """
        if self.n > self.init_data.size:
            print(
                "Warning : the algorithm seems to have already been run, you \
            should initialize before running again"
            )
            return {}

        # list of the thresholds
        th = []
        alarm = []
        # Loop over the stream
        for i in tqdm.tqdm(range(self.data.size)):

            if not dynamic:
                if self.data[i] > self.init_threshold and with_alarm:
                    self.extreme_quantile = self.init_threshold
                    alarm.append(i)
            else:
                # If the observed value exceeds the current threshold (alarm case)
                if self.data[i] > self.extreme_quantile:
                    # if we want to alarm, we put it in the alarm list
                    if with_alarm:
                        alarm.append(i)
                    # otherwise we add it in the peaks
                    else:
                        self.peaks = np.append(self.peaks, self.data[i] - self.init_threshold)
                        # self.peaks = self.peaks[1:]
                        self.Nt += 1
                        self.n += 1
                        # and we update the thresholds

                        g, s, l = self._grimshaw()
                        self.extreme_quantile = self._quantile(g, s)

                # case where the value exceeds the initial threshold but not the alarm ones
                elif self.data[i] > self.init_threshold:
                    # we add it in the peaks
                    self.peaks = np.append(self.peaks, self.data[i] - self.init_threshold)
                    # self.peaks = self.peaks[1:]
                    self.Nt += 1
                    self.n += 1
                    # and we update the thresholds

                    g, s, l = self._grimshaw()
                    self.extreme_quantile = self._quantile(g, s)
                else:
                    self.n += 1

            th.append(self.extreme_quantile)  # thresholds record

        return {"thresholds": th, "alarms": alarm}

    def plot(self, run_results, with_alarm=True):
        """
        Plot the results of given by the run

        Parameters
        ----------
        run_results : dict
                results given by the 'run' method
        with_alarm : bool
                (default = True) If True, alarms are plotted.
        Returns
        ----------
        list
                list of the plots

        """
        x = range(self.data.size)
        K = run_results.keys()

        (ts_fig,) = plt.plot(x, self.data, color=air_force_blue)
        fig = [ts_fig]

        if "thresholds" in K:
            th = run_results["thresholds"]
            (th_fig,) = plt.plot(x, th, color=deep_saffron, lw=2, ls="dashed")
            fig.append(th_fig)

        if with_alarm and ("alarms" in K):
            alarm = run_results["alarms"]
            al_fig = plt.scatter(alarm, self.data[alarm], color="red")
            fig.append(al_fig)

        plt.xlim((0, self.data.size))

        return fig


"""
============================ UPPER & LOWER BOUNDS =============================
"""

```
```anomaly_detection/cgnn/utils.py
import pandas as pd
import numpy as np
import torch
from torch.utils.data import Dataset, SubsetRandomSampler


class SlidingWindowDataset(Dataset):
    def __init__(self, data, window, target_dim=None, horizon=1):
        self.data = data
        self.window = window
        self.target_dim = target_dim
        self.horizon = horizon

    def __getitem__(self, index):
        x = self.data[index: index + self.window]
        y = self.data[index + self.window: index + self.window + self.horizon]
        return x, y

    def __len__(self):
        return len(self.data) - self.window


def create_data_loaders(train_dataset, batch_size, val_split=0.1, shuffle=True):
    train_loader, val_loader, test_loader = None, None, None
    print(train_dataset)
    if val_split == 0.0:
        print(f"train_size: {len(train_dataset)}")
        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)

    else:
        dataset_size = len(train_dataset)
        indices = list(range(dataset_size))
        split = int(np.floor(val_split * dataset_size))
        if shuffle:
            np.random.shuffle(indices)
        train_indices, val_indices = indices[split:], indices[:split]

        train_sampler = SubsetRandomSampler(train_indices)
        valid_sampler = SubsetRandomSampler(val_indices)

        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler)
        val_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, sampler=valid_sampler)

        print(f"train_size: {len(train_indices)}")
        print(f"validation_size: {len(val_indices)}")

    return train_loader, val_loader, test_loader


def load(model, PATH, device="cpu"):
    """
    Loads the model's parameters from the path mentioned
    :param PATH: Should contain pickle file
    """
    model.load_state_dict(torch.load(PATH, map_location=device))


def adjust_anomaly_scores(scores, dataset, is_train, lookback):
    """
    Method for MSL and SMAP where channels have been concatenated as part of the preprocessing
    :param scores: anomaly_scores
    :param dataset: name of dataset
    :param is_train: if scores is from train set
    :param lookback: lookback (window size) used in model
    """

    # Remove errors for time steps when transition to new channel (as this will be impossible for model to predict)
    if dataset.upper() not in ['SMAP', 'MSL']:
        return scores

    adjusted_scores = scores.copy()
    if is_train:
        md = pd.read_csv(f'./datasets/data/{dataset.lower()}_train_md.csv')
    else:
        md = pd.read_csv('./datasets/data/labeled_anomalies.csv')
        md = md[md['spacecraft'] == dataset.upper()]

    md = md[md['chan_id'] != 'P-2']

    # Sort values by channel
    md = md.sort_values(by=['chan_id'])

    # Getting the cumulative start index for each channel
    sep_cuma = np.cumsum(md['num_values'].values) - lookback
    sep_cuma = sep_cuma[:-1]
    buffer = np.arange(1, 20)
    i_remov = np.sort(np.concatenate((sep_cuma, np.array([i+buffer for i in sep_cuma]).flatten(),
                                      np.array([i-buffer for i in sep_cuma]).flatten())))
    i_remov = i_remov[(i_remov < len(adjusted_scores)) & (i_remov >= 0)]
    i_remov = np.sort(np.unique(i_remov))
    if len(i_remov) != 0:
        adjusted_scores[i_remov] = 0

    # Normalize each concatenated part individually
    sep_cuma = np.cumsum(md['num_values'].values) - lookback
    s = [0] + sep_cuma.tolist()
    for c_start, c_end in [(s[i], s[i+1]) for i in range(len(s)-1)]:
        e_s = adjusted_scores[c_start: c_end+1]

        e_s = (e_s - np.min(e_s))/(np.max(e_s) - np.min(e_s))
        adjusted_scores[c_start: c_end+1] = e_s

    return adjusted_scores
```
```anomaly_detection/cgnn/requirement.txt
blinker==1.8.2
certifi==2024.6.2
charset-normalizer==3.3.2
click==8.1.7
contourpy==1.2.1
cycler==0.12.1
filelock==3.14.0
Flask==3.0.3
Flask-Cors==4.0.1
fonttools==4.52.4
fsspec==2024.5.0
idna==3.7
itsdangerous==2.2.0
Jinja2==3.1.4
kiwisolver==1.4.5
MarkupSafe==2.1.5
matplotlib==3.9.0
mpmath==1.3.0
networkx==3.3
numpy==1.26.4
packaging==24.0
pandas==2.2.2
pillow==10.3.0
pyparsing==3.1.2
python-dateutil==2.9.0.post0
pytz==2024.1
requests==2.32.3
scipy==1.13.1
six==1.16.0
sympy==1.12
torch==2.2.2
tqdm==4.66.4
typing_extensions==4.12.0
tzdata==2024.1
urllib3==2.2.1
Werkzeug==3.0.3
```

```anomaly_detection/crca/app.py
# app.py
from flask import Flask, request, jsonify
import logging
import json
import traceback
import shutil
from celery import Celery
import pandas as pd
import multiprocessing
from flask_cors import CORS

from config import set_config, get_config, set_initial_config
from tasks import run_crca_task

multiprocessing.set_start_method('spawn', force=True)

app = Flask(__name__)
CORS(app, resources={r"/*": {"origins": "*"}})

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
app.config.update(
    CELERY_BROKER_URL='redis://redis:6379/1',
    CELERY_RESULT_BACKEND='redis://redis:6379/1'
)

celery = Celery(app.import_name, backend=app.config['CELERY_RESULT_BACKEND'], broker=app.config['CELERY_BROKER_URL'])
celery.conf.update(app.config)


@app.route('/crca', methods=['POST'])
def crca():
    try:
        logging.info("Received request: %s", request)
        crca_file = request.files['crca_file']
        crca_data = pd.read_csv(crca_file, header=None)
        crca_data_json = crca_data.to_json(orient='split')  # Convert DataFrame to JSON string
        crca_info_json = request.form.get('crca_info')
        crca_info = json.loads(crca_info_json)
        task = run_crca_task.apply_async(args=[crca_data_json, crca_info])
        return jsonify({"task_id": task.id}), 202
    except Exception as e:
        logger.error(traceback.format_exc())
        return jsonify({"error": str(e)}), 500


@app.route('/get_status/<task_id>', methods=['GET'])
def get_status(task_id):
    task = run_crca_task.AsyncResult(task_id)
    print(task.state)
    if task.state == 'PENDING':
        response = {
            'state': task.state,
            'status': 'Pending...'
        }
    elif task.state != 'FAILURE':
        response = {
            'state': task.state,
            'result': task.result
        }
    else:
        response = {
            'state': task.state,
            'status': str(task.info)
        }
    return jsonify(response)


@app.route('/results/<task_id>', methods=['GET'])
def get_results(task_id):
    try:
        with open('results/'+task_id+'/crca_results.json', 'r') as f:
            data = json.load(f)
        return jsonify(data), 200
    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route('/delete_results/<task_id>', methods=['DELETE'])
def delete_results(task_id):
    try:
        shutil.rmtree('results/'+task_id)
        return jsonify({"success"}), 200
    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route('/get_config', methods=['GET'])
def get_config_route():
    try:
        config = get_config()
        return jsonify(config), 200
    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route('/update_config', methods=['POST'])
def update_config():
    """
    Updates the configuration for the monitoring application.
    """
    new_config = request.json
    try:
        set_config(new_config)
        return jsonify({"status": "success", "message": "Configuration updated successfully"}), 200
    except Exception as e:
        return jsonify({"status": "error", "message": str(e)}), 500


if __name__ == '__main__':
    set_initial_config()
    app.run(debug=True, host='0.0.0.0', port=5023)

```
```anomaly_detection/crca/config.py
# config.py
import os
import json

config_file_path = "config.json"


def get_config():
    with open(config_file_path, 'r') as file:
        config = json.load(file)
    return config


def set_config(new_config=None):
    # Update the configuration file with new settings
    config = get_config()
    if new_config:
        config.update(new_config)
    with open(config_file_path, 'w') as file:
        json.dump(config, file)


def set_initial_config():
    config = {
        "arg_eta": int(os.getenv("ARG_ETA", 10)),
        "arg_iterations": int(os.getenv("ARG_ITERATIONS", 1)),
        "arg_gamma": float(os.getenv("ARG_GAMMA", 0.25)),
        "epochs": int(os.getenv("EPOCHS", 500)),
        "batch_size": int(os.getenv("BATCH_SIZE", 50)),
        "lr": float(os.getenv("LR", 1e-3)),
        "x_dims": int(os.getenv("X_DIMS", 1)),
        "z_dims": int(os.getenv("Z_DIMS", 1)),
        "optimizer": os.getenv("OPTIMIZER", "Adam"),
        "graph_threshold": float(os.getenv("GRAPH_THRESHOLD", 0.3)),
        "tau_A": float(os.getenv("TAU_A", 0.0)),
        "lambda_A": float(os.getenv("LAMBDA_A", 0.0)),
        "c_A": int(os.getenv("C_A", 1)),
        "use_A_connect_loss": int(os.getenv("USE_A_CONNECT_LOSS", 0)),
        "use_A_positiver_loss": int(os.getenv("USE_A_POSITIVER_LOSS", 0)),
        "seed": int(os.getenv("SEED", 42)),
        "encoder_hidden": int(os.getenv("ENCODER_HIDDEN", 64)),
        "decoder_hidden": int(os.getenv("DECODER_HIDDEN", 64)),
        "temp": float(os.getenv("TEMP", 0.5)),
        "k_max_iter": float(os.getenv("K_MAX_ITER", 1e2)),
        "encoder": os.getenv("ENCODER", "mlp"),
        "decoder": os.getenv("DECODER", "mlp"),
        "no_factor": bool(os.getenv("NO_FACTOR", False)),
        "encoder_dropout": float(os.getenv("ENCODER_DROPOUT", 0.0)),
        "decoder_dropout": float(os.getenv("DECODER_DROPOUT", 0.0)),
        "h_tol": float(os.getenv("H_TOL", 1e-8)),
        "lr_decay": int(os.getenv("LR_DECAY", 200)),
        "gamma": float(os.getenv("GAMMA", 1.0)),
        "prior": bool(os.getenv("PRIOR", False))
    }
    with open(config_file_path, 'w') as file:
        json.dump(config, file)

```
```anomaly_detection/crca/crca.py
import os
import time
import torch
import math
import warnings
import base64
import json
import matplotlib
import logging
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import networkx as nx
import torch.optim as optim
import torch.nn.functional as F

from torch.autograd import Variable
from torch.optim import lr_scheduler
from io import BytesIO
from sknetwork.ranking import PageRank

from utils import (matrix_poly, get_triu_offdiag_indices, get_tril_offdiag_indices,
                   nll_gaussian, kl_gaussian_sem, A_connect_loss, A_positive_loss)
from modules import MLPEncoder, MLPDecoder, SEMEncoder, SEMDecoder
from config import get_config
matplotlib.use("Agg")
warnings.filterwarnings("ignore")


def run_crca(crca_data, crca_info, task_id, parallel=False):
    try:
        iterations_arg = get_config()['arg_iterations']

        results = crca_sequential(crca_data, iterations_arg)

        df_concat = pd.concat([result[0] for result in results])
        image_base64 = [result[1] for result in results]
        cumulative_df = calculate_average(df_concat)
        name_matched_df = match_names(cumulative_df, crca_info)
        response_data = return_json_object(name_matched_df, image_base64, crca_info, task_id)

        return response_data
    except Exception as e:
        logging.error("Error in run_crca: %s", str(e))
        raise e


def return_json_object(df, image_base64, crca_info, task_id):
    csv_payload = df.to_csv(index=False)
    response_data = {
        'containers': crca_info['data']['containers'],
        'metrics': crca_info['data']['metrics'],
        'ranking': csv_payload,
        'graph_image': image_base64
    }
    path = 'results/'+task_id
    if not os.path.isdir(path):
        os.mkdir(path)
    with open(path+'/crca_results.json', 'w') as f:
        json.dump(response_data, f, indent=2)


def match_names(df_concat, crca_info):
    df_concat['index'] = df_concat['column_nr'].astype(int)

    selected_containers = crca_info['data']['containers']
    selected_metrics = crca_info['data']['metrics']
    column_names = [f"{container}_{metric}" for container in selected_containers for metric in selected_metrics]

    name_matched_df = df_concat.copy()
    name_matched_df['metric'] = name_matched_df['column_nr'].apply(lambda x: column_names[x])
    name_matched_df = name_matched_df[['metric', 'index', 'score']]
    logging.info("Matched column names: %s", name_matched_df)
    return name_matched_df


def crca_sequential(data_arg, iterations_arg):
    try:
        results = []
        for i in range(iterations_arg):
            logging.info(f"Running iteration {i + 1} of {iterations_arg}")
            result = crca(data_arg)
            results.append(result)
        return results
    except Exception as e:
        logging.error("Error in crca_sequential: %s", str(e))
        raise e


def calculate_average(df):
    cumulative_df = df.groupby('column_nr')['score'].mean().reset_index().sort_values(by='score', ascending=False)
    return cumulative_df


def crca(data_arg):
    try:
        config = get_config()
        gamma_arg = config['arg_gamma']
        eta_arg = config['arg_eta']
        data = data_arg
        data_sample_size = data.shape[0]
        data_variable_size = data.shape[1]

        config['cuda'] = torch.cuda.is_available()
        config['factor'] = not config['no_factor']

        # ================================================
        # get data: experiments = {synthetic SEM, ALARM}
        # ================================================
        train_data = data

        # ===================================
        # load modules
        # ===================================
        # Generate off-diagonal interaction graph

        # add adjacency matrix A
        num_nodes = data_variable_size
        adj_A = np.zeros((num_nodes, num_nodes))

        if config['encoder'] == 'mlp':
            encoder = MLPEncoder(data_variable_size * config['x_dims'],
                                 config['x_dims'],
                                 config['encoder_hidden'],
                                 int(config['z_dims']),
                                 adj_A,
                                 batch_size=config['batch_size'],
                                 do_prob=config['encoder_dropout'],
                                 factor=config['factor']).double()
        elif config['encoder'] == 'sem':
            encoder = SEMEncoder(data_variable_size * config['x_dims'],
                                 config['encoder_hidden'],
                                 int(config['z_dims']),
                                 adj_A,
                                 batch_size=config['batch_size'],
                                 do_prob=config['encoder_dropout'],
                                 factor=config['factor']).double()

        if config['decoder'] == 'mlp':
            decoder = MLPDecoder(data_variable_size * config['x_dims'],
                                 config['z_dims'],
                                 config['x_dims'],
                                 encoder,
                                 data_variable_size=data_variable_size,
                                 batch_size=config['batch_size'],
                                 n_hid=config['decoder_hidden'],
                                 do_prob=config['decoder_dropout']).double()
        elif config['decoder'] == 'sem':
            decoder = SEMDecoder(data_variable_size * config['x_dims'],
                                 config['z_dims'],
                                 2,
                                 encoder,
                                 data_variable_size=data_variable_size,
                                 batch_size=config['batch_size'],
                                 n_hid=config['decoder_hidden'],
                                 do_prob=config['decoder_dropout']).double()

        # ===================================
        # set up training parameters
        # ===================================
        if config['optimizer'] == 'Adam':
            optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=config['lr'])
        elif config['optimizer'] == 'LBFGS':
            optimizer = optim.LBFGS(list(encoder.parameters()) + list(decoder.parameters()), lr=config['lr'])
        elif config['optimizer'] == 'SGD':
            optimizer = optim.SGD(list(encoder.parameters()) + list(decoder.parameters()), lr=config['lr'])

        scheduler = lr_scheduler.StepLR(optimizer, step_size=config['lr_decay'],
                                        gamma=config['gamma'])

        # Linear indices of an upper triangular mx, used for acc calculation
        triu_indices = get_triu_offdiag_indices(data_variable_size)
        tril_indices = get_tril_offdiag_indices(data_variable_size)

        if config['prior']:
            prior = np.array([0.91, 0.03, 0.03, 0.03])  # hard coded for now
            print("Using prior")
            print(prior)
            log_prior = torch.DoubleTensor(np.log(prior))
            log_prior = torch.unsqueeze(log_prior, 0)
            log_prior = torch.unsqueeze(log_prior, 0)
            log_prior = Variable(log_prior)

            if config['cuda']:
                log_prior = log_prior.cuda()

        if config['cuda']:
            encoder.cuda()
            decoder.cuda()
            triu_indices = triu_indices.cuda()
            tril_indices = tril_indices.cuda()

        # compute constraint h(A) value
        def _h_A(A, m):
            expm_A = matrix_poly(A*A, m)
            h_A = torch.trace(expm_A) - m
            return h_A

        prox_plus = torch.nn.Threshold(0., 0.)

        def stau(w, tau):
            w1 = prox_plus(torch.abs(w)-tau)
            return torch.sign(w)*w1

        def update_optimizer(optimizer, original_lr, c_A):
            '''related LR to c_A, whenever c_A gets big, reduce LR proportionally'''
            MAX_LR = 1e-2
            MIN_LR = 1e-4

            estimated_lr = original_lr / (math.log10(c_A) + 1e-10)
            if estimated_lr > MAX_LR:
                lr = MAX_LR
            elif estimated_lr < MIN_LR:
                lr = MIN_LR
            else:
                lr = estimated_lr

            # set LR
            for parame_group in optimizer.param_groups:
                parame_group['lr'] = lr

            return optimizer, lr

        # ===================================
        # training:
        # ===================================
        def train(epoch, best_val_loss, lambda_A, c_A, optimizer):
            nll_train = []
            kl_train = []
            mse_train = []

            encoder.train()
            decoder.train()
            scheduler.step()

            # update optimizer
            optimizer, lr = update_optimizer(optimizer, config['lr'], c_A)

            for i in range(1):
                data = train_data[i*data_sample_size:(i+1)*data_sample_size]
                data = torch.tensor(data.to_numpy().reshape(data_sample_size, data_variable_size, 1))
                if config['cuda']:
                    data = data.cuda()
                data = Variable(data).double()

                optimizer.zero_grad()

                enc_x, logits, origin_A, adj_A_tilt_encoder, z_gap, z_positive, myA, Wa = encoder(data)
                edges = logits
                dec_x, output, adj_A_tilt_decoder = decoder(data, edges, data_variable_size * config['x_dims'],
                                                            origin_A, adj_A_tilt_encoder, Wa)

                if torch.sum(output != output):
                    print('nan error\n')

                target = data
                preds = output
                variance = 0.

                # reconstruction accuracy loss
                loss_nll = nll_gaussian(preds, target, variance)

                # KL loss
                loss_kl = kl_gaussian_sem(logits)

                # ELBO loss:
                loss = loss_kl + loss_nll
                # add A loss
                one_adj_A = origin_A
                sparse_loss = config['tau_A'] * torch.sum(torch.abs(one_adj_A))

                # other loss term
                if config['use_A_connect_loss']:
                    connect_gap = A_connect_loss(one_adj_A, config['graph_threshold, z_gap'])
                    loss += lambda_A * connect_gap + 0.5 * c_A * connect_gap * connect_gap

                if config['use_A_positiver_loss']:
                    positive_gap = A_positive_loss(one_adj_A, z_positive)
                    loss += .1 * (lambda_A * positive_gap + 0.5 * c_A * positive_gap * positive_gap)

                # compute h(A)
                h_A = _h_A(origin_A, data_variable_size)
                loss += lambda_A * h_A + 0.5 * c_A * h_A * h_A + 100. * torch.trace(origin_A*origin_A) + sparse_loss

                loss.backward()
                loss = optimizer.step()

                myA.data = stau(myA.data, config['tau_A']*lr)

                if torch.sum(origin_A != origin_A):
                    print('nan error\n')

                # compute metrics
                graph = origin_A.data.clone().cpu().numpy()
                graph[np.abs(graph) < config['graph_threshold']] = 0

                mse_train.append(F.mse_loss(preds, target).item())
                nll_train.append(loss_nll.item())
                kl_train.append(loss_kl.item())

            return np.mean(np.mean(kl_train) + np.mean(nll_train)), np.mean(nll_train), np.mean(mse_train), graph, origin_A

        # ===================================
        # main
        # ===================================

        gamma = gamma_arg
        eta = eta_arg

        best_ELBO_loss = np.inf
        best_NLL_loss = np.inf
        best_MSE_loss = np.inf
        best_epoch = 0
        # optimizer step on hyparameters
        c_A = config['c_A']
        lambda_A = config['lambda_A']
        h_A_new = torch.tensor(1.)
        h_tol = config['h_tol']
        k_max_iter = int(config['k_max_iter'])
        h_A_old = np.inf

        E_loss = []
        N_loss = []
        M_loss = []
        start_time = time.time()
        try:
            for step_k in range(k_max_iter):
                print("step_k: ", step_k, " ( of max", k_max_iter, ")")
                while c_A < 1e+20:
                    for epoch in range(config['epochs']):
                        ELBO_loss, NLL_loss, MSE_loss, graph, origin_A = train(epoch, best_ELBO_loss,
                                                                               lambda_A, c_A, optimizer)
                        E_loss.append(ELBO_loss)
                        N_loss.append(NLL_loss)
                        M_loss.append(MSE_loss)
                        if ELBO_loss < best_ELBO_loss:
                            best_ELBO_loss = ELBO_loss
                            best_epoch = epoch

                        if NLL_loss < best_NLL_loss:
                            best_NLL_loss = NLL_loss
                            best_epoch = epoch

                        if MSE_loss < best_MSE_loss:
                            best_MSE_loss = MSE_loss
                            best_epoch = epoch

                    if ELBO_loss > 2 * best_ELBO_loss:
                        break

                    # update parameters
                    A_new = origin_A.data.clone()
                    h_A_new = _h_A(A_new, data_variable_size)
                    if h_A_new.item() > gamma * h_A_old:
                        c_A *= eta
                    else:
                        break

                # update parameters
                h_A_old = h_A_new.item()
                lambda_A += c_A * h_A_new.item()

                if h_A_new.item() <= h_tol:
                    break

            print("Steps: {:04d}".format(step_k))
            print("Best Epoch: {:04d}".format(best_epoch))

            graph = origin_A.data.clone().cpu().numpy()
            graph[np.abs(graph) < 0.1] = 0
            graph[np.abs(graph) < 0.2] = 0
            graph[np.abs(graph) < 0.3] = 0

        except KeyboardInterrupt:
            print('Done!')

        end_time = time.time()
        print("Time spent: ", end_time-start_time)

        adj = graph
        org_G = nx.from_numpy_matrix(adj, parallel_edges=True, create_using=nx.DiGraph)
        pos = nx.circular_layout(org_G)
        plt.figure(figsize=(8, 6))
        nx.draw(org_G, pos=pos, with_labels=True, node_color='skyblue', edge_color='k')

        # Convert plot to PNG image byte data
        img_data = BytesIO()
        plt.savefig(img_data, format='png')
        img_data.seek(0)

        # Encode PNG image to base64 string
        image_base64 = base64.b64encode(img_data.getvalue()).decode('utf-8')

        # PageRank
        pagerank = PageRank()
        scores = pagerank.fit_transform(np.abs(adj.T))

        score_dict = {}
        for i, s in enumerate(scores):
            score_dict[i] = s
        sorted_dict = sorted(score_dict.items(), key=lambda item: item[1], reverse=True)
        for i in range(0, len(sorted_dict), 3):
            # Print the current index and the corresponding value if the index is within the bounds of the list
            if i < len(sorted_dict):
                print(i+1, sorted_dict[i], end="\t")
            if i+1 < len(sorted_dict):
                print(i+2, sorted_dict[i+1], end="\t")
            if i+2 < len(sorted_dict):
                print(i+3, sorted_dict[i+2])

        # save scores to csv file
        df = pd.DataFrame(sorted_dict)
        df.columns = ['column_nr', 'score']

        return df, image_base64
    except Exception as e:
        logging.error("Error in crca: %s", str(e))
        raise e

```
```anomaly_detection/crca/graph.py
"""
Define the interface for algorithms to access relations
"""
from abc import ABC
from typing import Dict
from typing import List
from typing import Set
from typing import Union

import networkx as nx
import json

ENCODING = "UTF-8"


def dump_json(filename: str, data):
    """
    Dump data into a json file
    """
    with open(filename, "w", encoding=ENCODING) as obj:
        json.dump(data, obj, ensure_ascii=False, indent=2)


def load_json(filename: str):
    """
    Load data from a json file
    """
    with open(filename, encoding=ENCODING) as obj:
        return json.load(obj)


class Node:
    """
    The element of a graph
    """

    def __init__(self, entity: str, metric: str):
        self._entity = entity
        self._metric = metric

    @property
    def entity(self) -> str:
        """
        Entity getter
        """
        return self._entity

    @property
    def metric(self) -> str:
        """
        Metric getter
        """
        return self._metric

    def asdict(self) -> Dict[str, str]:
        """
        Serialized as a dict
        """
        return {"entity": self._entity, "metric": self._metric}

    def __eq__(self, obj: object) -> bool:
        if isinstance(obj, Node):
            return self.entity == obj.entity and self.metric == obj.metric
        return False

    def __hash__(self) -> int:
        return hash((self.entity, self.metric))

    def __repr__(self) -> str:
        return f"Node{(self.entity, self.metric)}"


class LoadingInvalidGraphException(Exception):
    """
    This exception indicates that Graph tries to load from a broken file
    """


class Graph(ABC):
    """
    The abstract interface to access relations
    """

    def __init__(self):
        self._nodes: Set[Node] = set()
        self._sorted_nodes: List[Set[Node]] = None

    def dump(self, filename: str) -> bool:
        # pylint: disable=no-self-use, unused-argument
        """
        Dump a graph into the given file

        Return whether the operation succeeds
        """
        return False

    @classmethod
    def load(cls, filename: str) -> Union["Graph", None]:
        # pylint: disable=unused-argument
        """
        Load a graph from the given file

        Returns:
        - A graph, if available
        - None, if dump/load is not supported
        - Raise LoadingInvalidGraphException if the file cannot be parsed
        """
        return None

    @property
    def nodes(self) -> Set[Node]:
        """
        Get the set of nodes in the graph
        """
        return self._nodes

    @property
    def topological_sort(self) -> List[Set[Node]]:
        """
        Sort nodes with parents first

        The graph specifies the parents of each node.
        """
        if self._sorted_nodes:
            return self._sorted_nodes

        degrees = {node: len(self.parents(node)) for node in self.nodes}

        nodes: List[Set[Node]] = []
        while degrees:
            minimum = min(degrees.values())
            node_set = {node for node, degree in degrees.items() if degree == minimum}
            nodes.append(node_set)
            for node in node_set:
                degrees.pop(node)
                for child in self.children(node):
                    if child in degrees:
                        degrees[child] -= 1

        self._sorted_nodes = nodes
        return nodes

    def children(self, node: Node, **kwargs) -> Set[Node]:
        """
        Get the children of the given node in the graph
        """
        raise NotImplementedError

    def parents(self, node: Node, **kwargs) -> Set[Node]:
        """
        Get the parents of the given node in the graph
        """
        raise NotImplementedError


class MemoryGraph(Graph):
    """
    Implement Graph with data in memory
    """

    def __init__(self, graph: nx.DiGraph):
        """
        graph: The whole graph
        """
        super().__init__()
        self._graph = graph
        self._nodes.update(self._graph.nodes)

    def dump(self, filename: str) -> bool:
        nodes: List[Node] = list(self._graph.nodes)
        node_indexes = {node: index for index, node in enumerate(nodes)}
        edges = [
            (node_indexes[cause], node_indexes[effect])
            for cause, effect in self._graph.edges
        ]
        data = dict(nodes=[node.asdict() for node in nodes], edges=edges)
        dump_json(filename=filename, data=data)

    @classmethod
    def load(cls, filename: str) -> Union["MemoryGraph", None]:
        data: dict = load_json(filename=filename)
        if "nodes" not in data or "edges" not in data:
            raise LoadingInvalidGraphException(filename)
        nodes: List[Node] = [Node(**node) for node in data["nodes"]]
        graph = nx.DiGraph()
        graph.add_nodes_from(nodes)
        graph.add_edges_from(
            (nodes[cause], nodes[effect]) for cause, effect in data["edges"]
        )
        return MemoryGraph(graph)

    def children(self, node: Node, **kwargs) -> Set[Node]:
        if not self._graph.has_node(node):
            return set()
        return set(self._graph.successors(node))

    def parents(self, node: Node, **kwargs) -> Set[Node]:
        if not self._graph.has_node(node):
            return set()
        return set(self._graph.predecessors(node))

```
```anomaly_detection/crca/modules.py
import torch
import torch.nn as nn
import torch.nn.functional as F

from torch.autograd import Variable
from utils import preprocess_adj_new, preprocess_adj_new1

_EPS = 1e-10


class MLPEncoder(nn.Module):
    """MLP encoder module."""
    def __init__(self, n_in, n_xdims, n_hid, n_out, adj_A, batch_size, do_prob=0., factor=True, tol=0.1):
        super(MLPEncoder, self).__init__()

        self.adj_A = nn.Parameter(Variable(torch.from_numpy(adj_A).double(), requires_grad=True))
        self.factor = factor

        self.Wa = nn.Parameter(torch.zeros(n_out), requires_grad=True)
        self.fc1 = nn.Linear(n_xdims, n_hid, bias=True)
        self.fc2 = nn.Linear(n_hid, n_out, bias=True)
        self.dropout_prob = do_prob
        self.batch_size = batch_size
        self.z = nn.Parameter(torch.tensor(tol))
        self.z_positive = nn.Parameter(torch.ones_like(torch.from_numpy(adj_A)).double())
        self.init_weights()

    def init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_normal_(m.weight.data)
            elif isinstance(m, nn.BatchNorm1d):
                m.weight.data.fill_(1)
                m.bias.data.zero_()

    def forward(self, inputs):
        if torch.sum(self.adj_A != self.adj_A):
            print('nan error \n')

        # to amplify the value of A and accelerate convergence.
        adj_A1 = torch.sinh(3.*self.adj_A)

        # adj_Aforz = I-A^T
        adj_Aforz = preprocess_adj_new(adj_A1)

        adj_A = torch.eye(adj_A1.size()[0]).double()
        H1 = F.relu((self.fc1(inputs)))
        x = (self.fc2(H1))
        logits = torch.matmul(adj_Aforz, x+self.Wa) - self.Wa

        return x, logits, adj_A1, adj_A, self.z, self.z_positive, self.adj_A, self.Wa


class SEMEncoder(nn.Module):
    """SEM encoder module."""
    def __init__(self, n_in, n_hid, n_out, adj_A, batch_size, do_prob=0., factor=True, tol=0.1):
        super(SEMEncoder, self).__init__()

        self.factor = factor
        self.adj_A = nn.Parameter(Variable(torch.from_numpy(adj_A).double(), requires_grad=True))
        self.dropout_prob = do_prob
        self.batch_size = batch_size

    def init_weights(self):
        nn.init.xavier_normal(self.adj_A.data)

    def forward(self, inputs):

        if torch.sum(self.adj_A != self.adj_A):
            print('nan error \n')

        adj_A1 = torch.sinh(3.*self.adj_A)

        # adj_A = I-A^T, adj_A_inv = (I-A^T)^(-1)
        adj_A = preprocess_adj_new((adj_A1))
        adj_A_inv = preprocess_adj_new1((adj_A1))

        meanF = torch.matmul(adj_A_inv, torch.mean(torch.matmul(adj_A, inputs), 0))
        logits = torch.matmul(adj_A, inputs-meanF)

        return inputs-meanF, logits, adj_A1, adj_A, self.z, self.z_positive, self.adj_A


class MLPDecoder(nn.Module):
    """MLP decoder module."""

    def __init__(self, n_in_node, n_in_z, n_out, encoder, data_variable_size, batch_size,  n_hid,
                 do_prob=0.):
        super(MLPDecoder, self).__init__()

        self.out_fc1 = nn.Linear(n_in_z, n_hid, bias=True)
        self.out_fc2 = nn.Linear(n_hid, n_out, bias=True)

        self.batch_size = batch_size
        self.data_variable_size = data_variable_size

        self.dropout_prob = do_prob

        self.init_weights()

    def init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_normal_(m.weight.data)
                m.bias.data.fill_(0.0)
            elif isinstance(m, nn.BatchNorm1d):
                m.weight.data.fill_(1)
                m.bias.data.zero_()

    def forward(self, inputs, input_z, n_in_node, origin_A, adj_A_tilt, Wa):

        # adj_A_new1 = (I-A^T)^(-1)
        adj_A_new1 = preprocess_adj_new1(origin_A)
        mat_z = torch.matmul(adj_A_new1, input_z+Wa)-Wa

        H3 = F.relu(self.out_fc1((mat_z)))
        out = self.out_fc2(H3)

        return mat_z, out, adj_A_tilt


class SEMDecoder(nn.Module):
    """SEM decoder module."""

    def __init__(self, n_in_node, n_in_z, n_out, encoder, data_variable_size, batch_size,  n_hid,
                 do_prob=0.):
        super(SEMDecoder, self).__init__()

        self.batch_size = batch_size
        self.data_variable_size = data_variable_size

        print('Using learned interaction net decoder.')

        self.dropout_prob = do_prob

    def forward(self, inputs, input_z, n_in_node, origin_A, adj_A_tilt, Wa):

        # adj_A_new1 = (I-A^T)^(-1)
        adj_A_new1 = preprocess_adj_new1(origin_A)
        mat_z = torch.matmul(adj_A_new1, input_z + Wa)
        out = mat_z

        return mat_z, out-Wa, adj_A_tilt

```
```anomaly_detection/crca/tasks.py
import os
from crca import run_crca
import logging
from celery import Celery
import pandas as pd

# Set the environment variable
os.environ['OBJC_DISABLE_INITIALIZE_FORK_SAFETY'] = 'YES'

logger = logging.getLogger(__name__)

celery = Celery('tasks', backend='redis://redis:6379/1', broker='redis://redis:6379/1')

# celery -A tasks worker --loglevel=info -P gevent


@celery.task(bind=True)
def run_crca_task(self, crca_data_json, crca_info):
    try:
        crca_data = pd.read_json(crca_data_json, orient='split')
        task_id = self.request.id

        run_crca(crca_data, crca_info, task_id)
        return "success"
    except Exception as e:
        logger.error(e)
        raise self.retry(exc=e, countdown=60)

```
```anomaly_detection/crca/utils.py
import numpy as np
import torch


def get_triu_indices(num_nodes):
    """Linear triu (upper triangular) indices."""
    ones = torch.ones(num_nodes, num_nodes)
    eye = torch.eye(num_nodes, num_nodes)
    triu_indices = (ones.triu() - eye).nonzero().t()
    triu_indices = triu_indices[0] * num_nodes + triu_indices[1]
    return triu_indices


def get_tril_indices(num_nodes):
    """Linear tril (lower triangular) indices."""
    ones = torch.ones(num_nodes, num_nodes)
    eye = torch.eye(num_nodes, num_nodes)
    tril_indices = (ones.tril() - eye).nonzero().t()
    tril_indices = tril_indices[0] * num_nodes + tril_indices[1]
    return tril_indices


def get_offdiag_indices(num_nodes):
    """Linear off-diagonal indices."""
    ones = torch.ones(num_nodes, num_nodes)
    eye = torch.eye(num_nodes, num_nodes)
    offdiag_indices = (ones - eye).nonzero().t()
    offdiag_indices = offdiag_indices[0] * num_nodes + offdiag_indices[1]
    return offdiag_indices


def get_triu_offdiag_indices(num_nodes):
    """Linear triu (upper) indices w.r.t. vector of off-diagonal elements."""
    triu_idx = torch.zeros(num_nodes * num_nodes)
    triu_idx[get_triu_indices(num_nodes)] = 1.
    triu_idx = triu_idx[get_offdiag_indices(num_nodes)]
    return triu_idx.nonzero()


def get_tril_offdiag_indices(num_nodes):
    """Linear tril (lower) indices w.r.t. vector of off-diagonal elements."""
    tril_idx = torch.zeros(num_nodes * num_nodes)
    tril_idx[get_tril_indices(num_nodes)] = 1.
    tril_idx = tril_idx[get_offdiag_indices(num_nodes)]
    return tril_idx.nonzero()


def kl_gaussian_sem(preds):
    mu = preds
    kl_div = mu * mu
    kl_sum = kl_div.sum()
    return (kl_sum / (preds.size(0)))*0.5


def nll_gaussian(preds, target, variance, add_const=False):
    mean1 = preds
    mean2 = target
    neg_log_p = variance + torch.div(torch.pow(mean1 - mean2, 2), 2.*np.exp(2. * variance))
    if add_const:
        const = 0.5 * torch.log(2 * torch.from_numpy(np.pi) * variance)
        neg_log_p += const
    return neg_log_p.sum() / (target.size(0))


def preprocess_adj_new(adj):
    if torch.cuda.is_available():
        adj_normalized = (torch.eye(adj.shape[0]).double().cuda() - (adj.transpose(0, 1)))
    else:
        adj_normalized = (torch.eye(adj.shape[0]).double() - (adj.transpose(0, 1)))
    return adj_normalized


def preprocess_adj_new1(adj):
    if torch.cuda.is_available():
        adj_normalized = torch.inverse(torch.eye(adj.shape[0]).double().cuda()-adj.transpose(0, 1))
    else:
        adj_normalized = torch.inverse(torch.eye(adj.shape[0]).double()-adj.transpose(0, 1))
    return adj_normalized


def matrix_poly(matrix, d):
    if torch.cuda.is_available():
        x = torch.eye(d).double().cuda() + torch.div(matrix, d)
    else:
        x = torch.eye(d).double() + torch.div(matrix, d)
    return torch.matrix_power(x, d)


# matrix loss: makes sure at least A connected to another parents for child
def A_connect_loss(A, tol, z):
    d = A.size()[0]
    loss = 0
    for i in range(d):
        loss += 2 * tol - torch.sum(torch.abs(A[:, i])) - torch.sum(torch.abs(A[i, :])) + z * z
    return loss


# element loss: make sure each A_ij > 0
def A_positive_loss(A, z_positive):
    result = - A + z_positive * z_positive
    loss = torch.sum(result)

    return loss

```
```anomaly_detection/crca/requirements.txt
pip==24.0
setuptools==69.2.0
wheel==0.43.0
numpy==1.26.4
scipy==1.12.0
networkx==2.6.3
requests==2.26.0
scikit_learn==1.4.1.post1
tqdm==4.62.3
matplotlib==3.5.1
cython==3.0.10
pytest-cov==5.0.0
pytest==8.1.1
pytest-runner==6.0.1
nose==1.3.7
ipykernel==6.29.4
jupyter_client==8.6.1
ipython==8.23.0
nbsphinx==0.9.3
Pygments==2.17.2
sphinx-rtd-theme
Sphinx==7.2.6
twine==5.0.0
torch==2.2.2
Flask==3.0.3
pandas==1.3.5
alabaster==0.7.16
amqp==5.2.0
appnope==0.1.4
asttokens==2.4.1
attrs==23.2.0
Babel==2.14.0
backcall==0.2.0
beautifulsoup4==4.12.3
billiard==4.2.0
bleach==6.1.0
blinker==1.7.0
celery==5.4.0
certifi==2024.2.2
charset-normalizer==2.0.12
click==8.1.7
click-didyoumean==0.3.1
click-plugins==1.1.1
click-repl==0.3.0
comm==0.2.2
coverage==7.4.4
cycler==0.12.1
debugpy==1.8.1
decorator==5.1.1
defusedxml==0.7.1
docopt==0.6.2
docutils==0.20.1
executing==2.0.1
fastjsonschema==2.19.1
filelock==3.13.4
Flask-Cors==4.0.1
fonttools==4.51.0
fsspec==2024.3.1
gevent==24.2.1
greenlet==3.0.3
idna==3.6
imagesize==1.4.1
importlib_metadata==7.1.0
iniconfig==2.0.0
itsdangerous==2.2.0
jaraco.classes==3.4.0
jaraco.context==5.3.0
jaraco.functools==4.0.0
jedi==0.19.1
Jinja2==3.1.3
joblib==1.4.0
jsonschema==4.21.1
jsonschema-specifications==2023.12.1
jupyter_core==5.7.2
jupyterlab_pygments==0.3.0
keyring==25.1.0
kiwisolver==1.4.5
kombu==5.3.7
markdown-it-py==3.0.0
MarkupSafe==2.1.5
matplotlib-inline==0.1.6
mdurl==0.1.2
mistune==3.0.2
more-itertools==10.2.0
mpmath==1.3.0
nbclient==0.10.0
nbconvert==7.16.3
nbformat==5.10.4
nest-asyncio==1.6.0
nh3==0.2.17
packaging==24.0
pandocfilters==1.5.1
parso==0.8.4
pexpect==4.9.0
pickleshare==0.7.5
pillow==10.3.0
pkginfo==1.10.0
platformdirs==4.2.0
pluggy==1.4.0
prometheus_client==0.20.0
prompt-toolkit==3.0.43
psutil==5.9.8
ptyprocess==0.7.0
pure-eval==0.2.2
pyparsing==3.1.2
python-dateutil==2.9.0.post0
pytz==2024.1
PyYAML==6.0.1
pyzmq==25.1.2
readme_renderer==43.0
redis==5.0.5
referencing==0.34.0
requests-toolbelt==1.0.0
rfc3986==2.0.0
rich==13.7.1
rpds-py==0.18.0
six==1.16.0
snowballstemmer==2.2.0
soupsieve==2.5
sphinxcontrib-applehelp==1.0.8
sphinxcontrib-devhelp==1.0.6
sphinxcontrib-htmlhelp==2.0.5
sphinxcontrib-jquery==4.1
sphinxcontrib-jsmath==1.0.1
sphinxcontrib-qthelp==1.0.7
sphinxcontrib-serializinghtml==1.1.10
stack-data==0.6.3
sympy==1.12
threadpoolctl==3.4.0
tinycss2==1.2.1
tornado==6.4
traitlets==5.14.2
typing_extensions==4.11.0
tzdata==2024.1
urllib3==1.26.18
vine==5.1.0
wcwidth==0.2.13
webencodings==0.5.1
Werkzeug==3.0.2
yarg==0.1.9
zipp==3.18.1
zope.event==5.0
zope.interface==6.4.post2
```

